<!DOCTYPE html><html lang="en" class="text-black bg-white dark:text-white dark:bg-black __variable_ac79ff __variable_8a4d12"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/ec1a1eae803b668e-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/f980ec13b5b5e554.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/images/efficiency/2_pruning/1_memory_power_consumption.png"/><link rel="preload" as="image" href="/images/efficiency/2_pruning/2_pruning_neurons_synapses.png"/><link rel="preload" as="image" href="/images/efficiency/2_pruning/6_pruning_granularities.png"/><link rel="preload" as="image" href="/images/efficiency/2_pruning/4_nvidia_sparsity_diagram.jpg"/><link rel="preload" as="image" href="/images/efficiency/2_pruning/5_structured_sparsity_pattern.png"/><link rel="preload" as="image" href="/images/efficiency/2_pruning/7_dense_sparse_matrix_nvidia.png"/><link rel="preload" as="image" href="/images/efficiency/2_pruning/8_element_pruning.png"/><link rel="stylesheet" href="/_next/static/css/6b33b3fa6fbfebf3.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/09dfadb69bdaa005.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-d43983cff62b5160.js"/><script src="/_next/static/chunks/eae8abb3-fefb6374c2cd47f6.js" async=""></script><script src="/_next/static/chunks/656-9d4786b79d33cc4a.js" async=""></script><script src="/_next/static/chunks/main-app-dbbdcec500ee1fe5.js" async=""></script><script src="/_next/static/chunks/798-0bc533209101da7e.js" async=""></script><script src="/_next/static/chunks/241-2e7d2ff137352cd4.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-452d005f2d15aea3.js" async=""></script><script src="/_next/static/chunks/app/layout-5ee8c40b1c377a89.js" async=""></script><title>AI Efficiency II: Pruning and Sparsity | ekdnam</title><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><meta property="og:title" content="AI Efficiency II: Pruning and Sparsity"/><meta property="og:url" content="https://portfolio-blog-starter.vercel.app/blog/2_ai_efficiency_pruning_sparsity"/><meta property="og:image" content="https://portfolio-blog-starter.vercel.app/og?title=AI%20Efficiency%20II%3A%20Pruning%20and%20Sparsity"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-01-31"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="AI Efficiency II: Pruning and Sparsity"/><meta name="twitter:image" content="https://portfolio-blog-starter.vercel.app/og?title=AI%20Efficiency%20II%3A%20Pruning%20and%20Sparsity"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="antialiased max-w-xl mx-4 mt-8 lg:mx-auto"><main class="flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0"><aside class="-ml-[8px] mb-16 tracking-tight"><div class="lg:sticky lg:top-20"><nav class="flex flex-row items-start relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative" id="nav"><div class="flex flex-row space-x-0 pr-10"><a class="transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1" href="/">home</a><a class="transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1" href="/blog">blog</a></div></nav></div></aside><section><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"AI Efficiency II: Pruning and Sparsity","datePublished":"2025-01-31","dateModified":"2025-01-31","image":"/og?title=AI%20Efficiency%20II%3A%20Pruning%20and%20Sparsity","url":"https://portfolio-blog-starter.vercel.app/blog/2_ai_efficiency_pruning_sparsity","author":{"@type":"Person","name":"My Portfolio"}}</script><h1 class="title font-semibold text-2xl tracking-tighter">AI Efficiency II: Pruning and Sparsity</h1><div class="flex justify-between items-center mt-2 mb-8 text-sm"><p class="text-sm text-neutral-600 dark:text-neutral-400">January 31, 2025</p></div><article class="prose"><h1 id="introduction"><a href="#introduction" class="anchor"></a>Introduction</h1>
<p>The  pursuit of accuracy in modern AI, particularly with the advent of Large Language Models (LLMs) and vision transformers, has led us down a path of ever-increasing model scale. Branwen (2020)<sup>9</sup>, often cited as justification, posits that larger models, trained on more data, exhibit improved performance. And indeed, empirical evidence largely supports this – up to a point. However, this scaling comes at a steep cost, most acutely felt in memory and energy consumption.</p>
<p>As models balloon in parameter count, so too does their memory footprint. Storing these massive models on disk is one challenge, but the real bottleneck emerges during inference and training. GPU memory, while expanding, has not kept pace with parameter proliferation. This requires constant data movement between High Bandwidth Memory (HBM) and on-chip SRAM, a process notoriously power-hungry. Horowitz (2014)<sup>1</sup> highlights this: energy expenditure for data movement can dwarf that of computation itself, especially across memory hierarchies. This <em>memory wall</em> (or perhaps, more accurately, <em>memory power draw</em>) threatens to make continued scaling economically and environmentally unsustainable.</p>
<!-- -->
<img src="/images/efficiency/2_pruning/1_memory_power_consumption.png" alt="Memory Power Consumption"/>
<sub>(Image from 2)</sub>
<p>Thus, the imperative for efficiency becomes very important. We must explore techniques to mitigate the resource demands of these gargantuan models without sacrificing, or ideally even improving, performance. Two primary avenues present themselves: pruning and quantization. Pruning focuses on reducing the number of parameters, aiming to induce sparsity – a state where a significant proportion of model weights are zero. Quantization, conversely, reduces the precision of the remaining parameters, shrinking their storage footprint. This post will focus on the former: pruning and sparsity in neural networks.</p>
<h1 id="pruning"><a href="#pruning" class="anchor"></a>Pruning</h1>
<p>Neural network pruning, at its core, is about selective weight removal. Formally, we can frame it as a constrained optimization problem:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo><mi>arg</mi><mo>⁡</mo><mi>min</mi><mo>⁡</mo></mo><msub><mi>W</mi><mi>p</mi></msub></msub><mtext> </mtext><mi>L</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">;</mo><msub><mi>W</mi><mi>p</mi></msub><mo stretchy="false">)</mo><mspace width="1em"></mspace><mtext>subject to</mtext><mspace width="1em"></mspace><mi mathvariant="normal">∥</mi><msub><mi>W</mi><mi>p</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>0</mn></msub><mo>&lt;</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">\mathop{\arg\min}_{W_p} \space L(X; W_p) \quad \text{subject to} \quad \|W_p\|_0 &lt; N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1915em;vertical-align:-0.4415em"></span><span class="mop"><span class="mop"><span class="mop">ar<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">min</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2342em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4415em"><span></span></span></span></span></span></span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:1em"></span><span class="mord text"><span class="mord">subject to</span></span><span class="mspace" style="margin-right:1em"></span><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></p>
<p>Where:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span> is the loss function guiding network training.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em"></span><span class="mord mathbf">x</span></span></span></span> represents the input data.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em"></span><span class="mord mathbf" style="margin-right:0.01597em">W</span></span></span></span> is the original, dense weight matrix.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mi mathvariant="bold">p</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{W_p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9722em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1611em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathbf mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span> denotes the pruned weight matrix.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><msub><mi mathvariant="bold">W</mi><mi mathvariant="bold">p</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\|\mathbf{W_p}\|_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord">∥</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1611em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathbf mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> is the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">ℓ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\ell_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> <em>norm</em>, counting the number of non-zero parameters in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">W</mi><mi mathvariant="bold">p</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{W_p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9722em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1611em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathbf mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span>.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span> is our target sparsity level - the maximum allowed non-zero weights.</li>
</ul>
<h1 id="pruning-strategies-neurons-vs-synapses-and-granularity"><a href="#pruning-strategies-neurons-vs-synapses-and-granularity" class="anchor"></a>Pruning Strategies: Neurons vs. Synapses &amp; Granularity</h1>
<p>Pruning can be broadly categorized by the granularity at which weights are removed. We can prune at the level of individual synapses (connections) or entire neurons (nodes).</p>
<p><strong>Neuron Pruning</strong>: This involves eliminating entire neurons, either within a layer (width pruning) or by removing layers altogether (depth pruning). The rationale is that some neurons may be redundant or contribute negligibly to the network&#x27;s overall function. Removing them simplifies the network architecture, potentially leading to faster inference and reduced memory usage. However, neuron pruning can be a more coarse-grained approach, potentially leading to larger accuracy drops if not performed judiciously.</p>
<p><strong>Synapse Pruning</strong>: Synapse pruning, conversely, targets individual connections (weights) between neurons. Weights with magnitudes below a certain threshold are set to zero. This induces sparsity at a finer granularity, potentially preserving more of the network&#x27;s representational capacity compared to aggressive neuron pruning. Synapse pruning can lead to unstructured sparsity (individual weights zeroed out) or structured sparsity (patterns of zeros enforced, as discussed later).</p>
<!-- -->
<img src="/images/efficiency/2_pruning/2_pruning_neurons_synapses.png" alt="Pruning Neurons vs. Synapses"/>
<sub>(Image from 3)</sub>
<p>Han et al. (2015)<sup>3</sup> demonstrated the effectiveness of iterative magnitude pruning. They trained a baseline network (AlexNet), then pruned small-magnitude weights, and crucially, fine-tuned the remaining weights. This iterative prune-and-fine-tune cycle proved highly effective.</p>
<p>The Lottery Ticket Hypothesis (Frankle &amp; Carbin, 2018<sup>12</sup>) posits that within a randomly initialized dense network, there exists a sparse subnetwork (<em>winning ticket</em>) that can be trained in isolation to achieve comparable or even better performance than the original dense network...(line too long; chars omitted)</p>
<p><strong>Evaluation Metrics Beyond Accuracy</strong></p>
<p>While accuracy preservation is crucial, evaluating pruned models should also consider compression ratio (sparsity level), inference speedup (latency reduction), and energy efficiency gains. Benchmarking across these metrics provides a more holistic picture of pruning effectiveness.</p>
<h1 id="granularities-of-pruning"><a href="#granularities-of-pruning" class="anchor"></a>Granularities of Pruning</h1>
<p>Beyond neuron vs. synapse, pruning can be further categorized by granularity, particularly relevant in Convolutional Neural Networks (CNNs). CNNs, with their kernel-based operations, offer various structural levels for pruning:</p>
<p><strong>Fine-grained (Element-wise) Pruning</strong>: Individual weights within kernels or weight matrices are pruned based on some criterion (e.g., magnitude). Offers the highest flexibility and potential compression ratio, but often results in unstructured sparsity, which is less hardware-friendly on standard GPUs.</p>
<p><strong>Pattern-based (N:M) Pruning</strong>: Enforces structured sparsity by requiring specific patterns of zeros within weight blocks (e.g., 2:4 sparsity, where in every block of 4 consecutive values, at least 2 must be zero). This is specifically designed to leverage hardware acceleration, such as NVIDIA&#x27;s sparse tensor cores, but may limit achievable sparsity compared to fine-grained pruning.</p>
<p><strong>Vector-based (Row/Column) Pruning</strong>: Prunes entire rows or columns in weight matrices, effectively removing neurons or filters. Strikes a balance between granularity and structure.</p>
<p><strong>Kernel-based Pruning</strong>: Removes entire kernels in convolutional layers. More aggressive than vector pruning, leading to simpler filters and reduced computation.</p>
<p><strong>Channel-based Pruning</strong>: Specifically for CNNs, removes entire channels (feature maps). This is often considered more structurally sound for CNNs as channels represent distinct features. Channel pruning can be seen as a form of neuron pruning at the feature map level.</p>
<!-- -->
<img src="/images/efficiency/2_pruning/6_pruning_granularities.png" alt="Pruning Granularities"/>
<sub>(Image from 2)</sub>
<p>The choice of pruning granularity involves trade-offs between compression ratio, hardware acceleration potential, and implementation complexity. Fine-grained pruning offers maximum flexibility but may not directly map to hardware speedups without specialized sparse compute units. Structured pruning, particularly pattern-based sparsity, is explicitly designed for hardware acceleration but might constrain the achievable sparsity level and require careful implementation.</p>
<h1 id="pruning-in-industry-nvidias-structured-sparsity"><a href="#pruning-in-industry-nvidias-structured-sparsity" class="anchor"></a>Pruning in Industry: NVIDIA&#x27;s Structured Sparsity</h1>
<p>NVIDIA&#x27;s Ampere and Hopper architectures [<sup>5</sup>, <sup>6</sup>] have brought structured sparsity to the forefront of hardware-accelerated AI inference. Their Tensor Cores, starting from the Ampere generation, leverage 2:4 structured sparsity to achieve up to 2x throughput gains while maintaining accuracy for matrix multiplications.</p>
<!-- -->
<img src="/images/efficiency/2_pruning/4_nvidia_sparsity_diagram.jpg" alt="NVIDIA Sparsity Diagram"/>
<sub>(Image from 4)</sub>
<h2 id="structured-sparsity-24-pattern"><a href="#structured-sparsity-24-pattern" class="anchor"></a>Structured Sparsity: 2:4 Pattern</h2>
<!-- -->
<img src="/images/efficiency/2_pruning/5_structured_sparsity_pattern.png" alt="Structured Sparsity Pattern"/>
<sub>(Image from 5)</sub>
<p>NVIDIA&#x27;s sparse Tensor Cores are optimized for 2:4 sparsity. This requires that within every group of four contiguous values in a weight tensor, at least two must be zero. This enforces a 50% sparsity rate. The concept generalizes to N:M sparsity, where in a block of M values, N must be zero. During storage, only the non-zero values and associated metadata (indices indicating their original positions) are kept, leading to compression. This is primarily applicable to fully connected and convolutional layers.</p>
<p>The performance gain arises because Sparse Tensor Cores operate only on the non-zero values. The metadata guides the hardware to fetch only the necessary corresponding values from the dense operand in a matrix multiplication, effectively skipping computations involving zeros <sup>6</sup>. For 2x sparsity, this can, ideally, halve the computation time for matrix multiplications, a core operation in deep learning.</p>
<!-- -->
<img src="/images/efficiency/2_pruning/7_dense_sparse_matrix_nvidia.png" alt="Dense Sparse Matrix NVIDIA"/>
<sub>(Image from 7)</sub>
<p>Consider a GEMM (Generalized Matrix Multiply) operation: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mi>A</mi><mo>×</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">C = A \times B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>M</mi><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{R}^{M \times K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span></span></span></span></span></span></span></span></span> is sparse (2:4 structured sparsity), and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>K</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">B \in \mathbb{R}^{K \times N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span></span></span></span></span></span></span></span> is dense. Sparse Tensor Cores accelerate this by effectively transforming the dense GEMM into a sparse one. While the logical dimensions of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span> remain <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">M \times K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span>, its compressed representation and the sparse hardware allow computation only on the non-zero elements of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span> and the corresponding rows of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span>. This hardware-level optimization is a significant driver for adopting structured sparsity in practical deployments.</p>
<p>However, it&#x27;s crucial to note that the theoretical 2x speedup is often not fully realized in practice due to overheads like metadata processing and memory access patterns. Empirical benchmarks are essential to quantify the actual gains in specific use cases.</p>
<p>This hardware-level optimization is a significant driver for adopting structured sparsity in practical deployments. However, it&#x27;s crucial to note that the theoretical 2x speedup is often not fully realized in practice due to overheads like metadata processing and memory access patterns. Empirical benchmarks are essential to quantify the actual gains in specific use cases.</p>
<h1 id="pruning-criteria"><a href="#pruning-criteria" class="anchor"></a>Pruning Criteria</h1>
<p>The efficacy of pruning relias on identifying and removing <em>unimportant</em> weights. Various criteria exist to assess weight importance. Magnitude-based pruning is a common and straightforward approach.</p>
<h2 id="magnitude-based-pruning"><a href="#magnitude-based-pruning" class="anchor"></a>Magnitude-Based Pruning</h2>
<p>The intuition is that weights with smaller absolute values contribute less to the network&#x27;s output and can be pruned.</p>
<h3 id="element-wise-pruning"><a href="#element-wise-pruning" class="anchor"></a>Element-wise Pruning</h3>
<p>The simplest form, where individual weights below a threshold are zeroed out.</p>
<!-- -->
<img src="/images/efficiency/2_pruning/8_element_pruning.png" alt="Element Pruning"/>
<sub>(Image from 2)</sub>
<h3 id="structural-pruning-with-norms"><a href="#structural-pruning-with-norms" class="anchor"></a>Structural Pruning with Norms</h3>
<p>For structured pruning (e.g., row, column, kernel), norms can be used to assess the <em>importance</em> of entire structures. Common norms include <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">l_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">l_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>, and generalized <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">l_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span> norms.</p>
<h4 id="object-object-norm"><a href="#object-object-norm" class="anchor"></a><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">l_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>-norm</h4>
<p>Importance is calculated as the sum of absolute values within a structural unit (eg: a row):</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Importance</mtext><mo>=</mo><msub><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>S</mi></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">\text{Importance} = \sum_{i \in S} | w_i | </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord text"><span class="mord">Importance</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0771em;vertical-align:-0.3271em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1786em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.05764em">S</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3271em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span></span></span></span> is the set of indices in the structural units.</p>
<h1 id="beyond-magnitude"><a href="#beyond-magnitude" class="anchor"></a>Beyond Magnitude</h1>
<h2 id="more-sophisticated-criteria"><a href="#more-sophisticated-criteria" class="anchor"></a>More Sophisticated Criteria</h2>
<p>While magnitude-based pruning is widely used due to its simplicity, it&#x27;s not necessarily the most effective criterion. More advanced methods consider:</p>
<p><strong>Gradient-based Pruning</strong></p>
<p>Weights with small gradients during training are considered less important. This directly relates to the weight&#x27;s contribution to loss reduction. Techniques like Optimal Brain Damage (LeCun et al., 1990<sup>10</sup>) and Optimal Brain Surgeon (Hassibi et al., 1993<sup>11</sup>) fall into this category, though they can be computationally expensive for large networks.</p>
<p><strong>Connection Sensitivity</strong></p>
<p>Methods that attempt to directly estimate the impact of removing a connection on the network&#x27;s output. This can involve approximating the Hessian of the loss function (second-order information) to assess sensitivity. Again, computationally demanding but potentially more accurate.</p>
<p><strong>Data-Driven/Activation-Based Pruning</strong></p>
<p>Pruning decisions based on the actual activations observed during inference on a dataset. For instance, neurons or channels with consistently low activations might be deemed less important and pruned.</p>
<h1 id="pruning-schedules-and-strategies"><a href="#pruning-schedules-and-strategies" class="anchor"></a>Pruning Schedules and Strategies:</h1>
<p><strong>One-Shot Pruning</strong></p>
<p>Prune the network once, based on a chosen criterion and sparsity target, then fine-tune the remaining weights. Simple but may be suboptimal for high sparsity levels.</p>
<p><strong>Iterative Pruning</strong></p>
<p>Repeated cycles of pruning and fine-tuning. As demonstrated by Han et al. 3, this often yields better results, allowing the network to adapt to sparsity more effectively. Schedules can be fixed (prune by X% every Y epochs) or adaptive.</p>
<p><strong>Pruning During Training</strong></p>
<p>Integrate pruning directly into the training process. Regularization terms can be added to the loss function to encourage sparsity during training itself. Less common than post-training pruning but an active research area.</p>
<p><strong>Dynamic Sparsity</strong></p>
<p>Sparsity patterns are not fixed but change during training or even during inference. This adds complexity but could potentially offer greater flexibility and efficiency.</p>
<h1 id="practical-considerations-and-open-questions"><a href="#practical-considerations-and-open-questions" class="anchor"></a>Practical Considerations &amp; Open Questions</h1>
<p>While pruning offers a promising route to AI efficiency, several practical aspects and open research questions remain:</p>
<p><strong>Software Tools and Libraries</strong></p>
<p>Frameworks like PyTorch and TensorFlow provide built-in modules and libraries to facilitate pruning. NVIDIA TensorRT 4 is crucial for deploying sparse models on NVIDIA hardware, optimizing for sparse tensor core utilization. However, tooling is still evolving, and seamless integration of complex pruning strategies can be challenging.</p>
<p><strong>Hyperparameter Tuning for Pruning</strong></p>
<p>Pruning introduces new hyperparameters: sparsity level, pruning schedule, fine-tuning epochs, pruning criterion thresholds, etc. Tuning these effectively can be non-trivial and often requires experimentation. Automated pruning hyperparameter search is an area of active research.</p>
<p><strong>Model Architectures and Pruning Sensitivity</strong></p>
<p>Different network architectures exhibit varying degrees of prune-friendliness. Some architectures are inherently more robust to pruning than others. Understanding architectural biases towards sparsity is crucial for effective pruning. Transformers, for instance, may respond differently to pruning compared to CNNs.</p>
<p><strong>Impact on Training Time</strong></p>
<p>While the goal is to reduce inference time, the pruning process itself (especially iterative pruning with fine-tuning) can add to the overall training time. Careful scheduling and efficient implementation are needed to mitigate this.</p>
<p><strong>Quantization and Pruning Synergy</strong></p>
<p>Pruning and quantization are often used in tandem. Applying quantization to already sparse models can further compress them and improve inference efficiency. Exploring the optimal combination and ordering of these techniques is an ongoing area of investigation.</p>
<p><strong>Pruning Large Language Models (LLMs)</strong></p>
<p>Pruning LLMs presents unique challenges due to their scale and the sensitivity of their performance to even small perturbations. Maintaining perplexity and generation quality after pruning is paramount. Techniques like attention head pruning, layer pruning, and specialized sparse training methods are actively being explored for LLMs.</p>
<p><strong>Lottery Ticket Hypothesis</strong></p>
<p>The Lottery Ticket Hypothesis (Frankle &amp; Carbin, 2018<sup>12</sup>) posits that within a randomly initialized dense network, there exists a sparse subnetwork (<em>winning ticket</em>) that can be trained in isolation to achieve comparable or even better performance than the original dense network. This suggests that sparsity is not just about compression but might be fundamental to efficient learning itself. Exploring lottery tickets and related sparse initialization strategies is a promising research direction.</p>
<p><strong>Evaluation Metrics Beyond Accuracy</strong></p>
<p>While accuracy preservation is crucial, evaluating pruned models should also consider compression ratio (sparsity level), inference speedup (latency reduction), and energy efficiency gains. Benchmarking across these metrics provides a more holistic picture of pruning effectiveness.</p>
<h1 id="future-directions"><a href="#future-directions" class="anchor"></a>Future Directions</h1>
<p>Research continues to push the boundaries of pruning. Areas of focus include developing more robust and automated pruning criteria, exploring dynamic sparsity, designing hardware-aware pruning algorithms, and understanding the theoretical underpinnings of sparsity in deep learning.</p>
<h1 id="conclusion-sparse-futures"><a href="#conclusion-sparse-futures" class="anchor"></a>Conclusion: Sparse Futures</h1>
<p>Pruning and sparsity offer a critical pathway to reconcile the insatiable demands of modern AI with the constraints of computational resources and energy efficiency. While challenges remain in terms of tooling, hyperparameter optimization, and achieving consistent and predictable performance gains, the potential benefits are undeniable. As hardware support for sparsity matures and research deepens our understanding of sparse neural networks, pruning is poised to become an indispensable technique in the toolkit of any AI practitioner aiming for both performance and sustainability. The pursuit of sparsity is not just about making models smaller; it&#x27;s about sculpting more efficient and elegant intelligence.</p>
<h1 id="acknowledgements"><a href="#acknowledgements" class="anchor"></a>Acknowledgements</h1>
<p>This blog series on AI efficiency is heavily indebted to Dr. Song Han&#x27;s course on TinyML and Efficient Deep Learning Computing<sup>8</sup>. The references cited and the broader field of efficient deep learning are built upon the work of numerous researchers and engineers.</p>
<p>I would like to extend my sincere gratitude to my colleagues for their invaluable insights and assistance throughout this research: Grok-2.0 <a target="_blank" rel="noopener noreferrer" href="https://x.ai/blog/grok-2">(xAI)</a>,
Gemini-2.0-Flash-Thinking-Exp-01-21 <a target="_blank" rel="noopener noreferrer" href="https://deepmind.google/technologies/gemini/flash-thinking/">(DeepMind)</a>, O1 <a target="_blank" rel="noopener noreferrer" href="https://openai.com/o1/">(OpenAI)</a>.</p>
<p>Their contributions have been pivotal to the success of this project, and I am deeply thankful for their collaboration.</p>
<h1 id="references"><a href="#references" class="anchor"></a>References</h1>
<ol>
<li>M. Horowitz, &quot;1.1 Computing&#x27;s energy problem (and what we can do about it),&quot; 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), San Francisco, CA, USA, 2014, pp. 10-14, doi: 10.1109/ISSCC.2014.6757323. https://ieeexplore.ieee.org/document/6757323</li>
<li>EfficientML.ai Lecture 3 - Pruning and Sparsity Part I (MIT 6.5940, Fall 2024). https://www.youtube.com/watch?v=EjsB0WgIfUM</li>
<li>Han, S., Mao, H., &amp; Dally, W. J. (2015). Learning both Weights and Connections for Efficient Neural Networks. Advances in Neural Information Processing Systems, 28. https://arxiv.org/abs/1506.02626</li>
<li>Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/</li>
<li>How Sparsity Adds Umph to AI Inference. https://blogs.nvidia.com/blog/sparsity-ai-inference/</li>
<li>Mishra, A., Albericio Latorre, J., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., &amp; Micikevicius, P. (2021). <em>Accelerating sparse deep neural networks</em> (arXiv:2104.08378 [cs.LG]). arXiv. https://arxiv.org/abs/2104.08378</li>
<li>Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/</li>
<li>Dr. Song Han&#x27;s course on TinyML and Efficient Deep Learning Computing. https://hanlab.mit.edu/courses/2024-fall-65940</li>
<li>Branwen, G. (2020). The Scaling Hypothesis. https://gwern.net/scaling-hypothesis</li>
<li>LeCun, Y., Denker, J., &amp; Solla, S. (1989). Optimal brain damage. In D. Touretzky (Ed.), Advances in Neural Information Processing Systems (Vol. 2, pp. ). Morgan-Kaufmann. https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf</li>
<li>Hassibi, B., &amp; Stork, D. (1992). Second order derivatives for network pruning: Optimal Brain Surgeon. In S. Hanson, J. Cowan, &amp; C. Giles (Eds.), Advances in Neural Information Processing Systems (Vol. 5, pp. ). Morgan-Kaufmann. https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf</li>
<li>Frankle, J., &amp; Carbin, M. (2019). The lottery ticket hypothesis: Finding sparse, trainable neural networks (arXiv:1803.03635 [cs.LG]). arXiv. https://arxiv.org/abs/1803.03635</li>
</ol></article></section><footer class="mb-16"><ul class="font-sm mt-8 flex flex-col space-x-0 space-y-2 text-neutral-600 md:flex-row md:space-x-4 md:space-y-0 dark:text-neutral-300"><li><a class="flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100" rel="noopener noreferrer" target="_blank" href="https://x.com/ekdnam"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">𝕏</p></a></li><li><a class="flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100" rel="noopener noreferrer" target="_blank" href="https://github.com/ekdnam"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">github</p></a></li><li><a class="flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100" rel="noopener noreferrer" target="_blank" href="https://linkedin.com/in/adityamandke"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">linkedin</p></a></li></ul><p class="mt-8 text-neutral-600 dark:text-neutral-300">© <!-- -->2025<!-- --> MIT Licensed</p></footer><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></main><script src="/_next/static/chunks/webpack-d43983cff62b5160.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/ec1a1eae803b668e-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/f980ec13b5b5e554.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/css/6b33b3fa6fbfebf3.css\",\"style\"]\n4:HL[\"/_next/static/css/09dfadb69bdaa005.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"5:I[6373,[],\"\"]\n8:I[2146,[],\"\"]\na:I[3545,[],\"\"]\nb:I[6798,[\"798\",\"static/chunks/798-0bc533209101da7e.js\",\"241\",\"static/chunks/241-2e7d2ff137352cd4.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-452d005f2d15aea3.js\"],\"\"]\nc:I[206,[\"798\",\"static/chunks/798-0bc533209101da7e.js\",\"185\",\"static/chunks/app/layout-5ee8c40b1c377a89.js\"],\"Analytics\"]\nd:I[1289,[\"798\",\"static/chunks/798-0bc533209101da7e.js\",\"185\",\"static/chunks/app/layout-5ee8c40b1c377a89.js\"],\"SpeedInsights\"]\nf:I[55,[],\"\"]\n9:[\"slug\",\"2_ai_efficiency_pruning_sparsity\",\"d\"]\n10:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L5\",null,{\"buildId\":\"LqnAkREHXUbULLy5EfZKv\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"blog\",\"2_ai_efficiency_pruning_sparsity\"],\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"2_ai_efficiency_pruning_sparsity\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"2_ai_efficiency_pruning_sparsity\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"2_ai_efficiency_pruning_sparsity\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L6\",\"$L7\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/09dfadb69bdaa005.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]],null],null]},[null,[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/6b33b3fa6fbfebf3.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"text-black bg-white dark:text-white dark:bg-black __variable_ac79ff __variable_8a4d12\",\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased max-w-xl mx-4 mt-8 lg:mx-auto\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"-ml-[8px] mb-16 tracking-tight\",\"children\":[\"$\",\"div\",null,{\"className\":\"lg:sticky lg:top-20\",\"children\":[\"$\",\"nav\",null,{\"className\":\"flex flex-row items-start relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative\",\"id\":\"nav\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-row space-x-0 pr-10\",\"children\":[[\"$\",\"$Lb\",\"/\",{\"href\":\"/\",\"className\":\"transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1\",\"children\":\"home\"}],[\"$\",\"$Lb\",\"/blog\",{\"href\":\"/blog\",\"className\":\"transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1\",\"children\":\"blog\"}]]}]}]}]}],[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"section\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-8 text-2xl font-semibold tracking-tighter\",\"children\":\"404 - Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":\"The page you are looking for does not exist.\"}]]}],\"notFoundStyles\":[]}],[\"$\",\"footer\",null,{\"className\":\"mb-16\",\"children\":[[\"$\",\"ul\",null,{\"className\":\"font-sm mt-8 flex flex-col space-x-0 space-y-2 text-neutral-600 md:flex-row md:space-x-4 md:space-y-0 dark:text-neutral-300\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"https://x.com/ekdnam\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"𝕏\"}]]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"https://github.com/ekdnam\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"github\"}]]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"https://linkedin.com/in/adityamandke\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"linkedin\"}]]}]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-neutral-600 dark:text-neutral-300\",\"children\":[\"© \",2025,\" MIT Licensed\"]}]]}],[\"$\",\"$Lc\",null,{}],[\"$\",\"$Ld\",null,{}]]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Le\"],\"globalErrorComponent\":\"$f\",\"missingSlots\":\"$W10\"}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"section\",null,{\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"suppressHydrationWarning\":true,\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"AI Efficiency II: Pruning and Sparsity\\\",\\\"datePublished\\\":\\\"2025-01-31\\\",\\\"dateModified\\\":\\\"2025-01-31\\\",\\\"image\\\":\\\"/og?title=AI%20Efficiency%20II%3A%20Pruning%20and%20Sparsity\\\",\\\"url\\\":\\\"https://portfolio-blog-starter.vercel.app/blog/2_ai_efficiency_pruning_sparsity\\\",\\\"author\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"My Portfolio\\\"}}\"}}],[\"$\",\"h1\",null,{\"className\":\"title font-semibold text-2xl tracking-tighter\",\"children\":\"AI Efficiency II: Pruning and Sparsity\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center mt-2 mb-8 text-sm\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-sm text-neutral-600 dark:text-neutral-400\",\"children\":\"January 31, 2025\"}]}],[\"$\",\"article\",null,{\"className\":\"prose\",\"children\":\"$L11\"}]]}]\ne:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"AI Efficiency II: Pruning and Sparsity | ekdnam\"}],[\"$\",\"meta\",\"3\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"4\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"AI Efficiency II: Pruning and Sparsity\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://portfolio-blog-starter.vercel.app/blog/2_ai_efficiency_pruning_sparsity\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image\",\"content\":\"https://portfolio-blog-starter.vercel.app/og?title=AI%20Efficiency%20II%3A%20Pruning%20and%20Sparsity\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"9\",{\"property\":\"article:published_time\",\"content\":\"2025-01-31\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"AI Efficiency II: Pruning and Sparsity\"}],[\"$"])</script><script>self.__next_f.push([1,"\",\"meta\",\"12\",{\"name\":\"twitter:image\",\"content\":\"https://portfolio-blog-starter.vercel.app/og?title=AI%20Efficiency%20II%3A%20Pruning%20and%20Sparsity\"}],[\"$\",\"meta\",\"13\",{\"name\":\"next-size-adjust\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,"11:[[\"$\",\"h1\",null,{\"id\":\"introduction\",\"children\":[[[\"$\",\"a\",\"link-introduction\",{\"href\":\"#introduction\",\"className\":\"anchor\"}]],\"Introduction\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The  pursuit of accuracy in modern AI, particularly with the advent of Large Language Models (LLMs) and vision transformers, has led us down a path of ever-increasing model scale. Branwen (2020)\",[\"$\",\"sup\",null,{\"children\":\"9\"}],\", often cited as justification, posits that larger models, trained on more data, exhibit improved performance. And indeed, empirical evidence largely supports this – up to a point. However, this scaling comes at a steep cost, most acutely felt in memory and energy consumption.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"As models balloon in parameter count, so too does their memory footprint. Storing these massive models on disk is one challenge, but the real bottleneck emerges during inference and training. GPU memory, while expanding, has not kept pace with parameter proliferation. This requires constant data movement between High Bandwidth Memory (HBM) and on-chip SRAM, a process notoriously power-hungry. Horowitz (2014)\",[\"$\",\"sup\",null,{\"children\":\"1\"}],\" highlights this: energy expenditure for data movement can dwarf that of computation itself, especially across memory hierarchies. This \",[\"$\",\"em\",null,{\"children\":\"memory wall\"}],\" (or perhaps, more accurately, \",[\"$\",\"em\",null,{\"children\":\"memory power draw\"}],\") threatens to make continued scaling economically and environmentally unsustainable.\"]}],\"\\n\",\"\\n\",[\"$\",\"img\",null,{\"src\":\"/images/efficiency/2_pruning/1_memory_power_consumption.png\",\"alt\":\"Memory Power Consumption\"}],\"\\n\",[\"$\",\"sub\",null,{\"children\":\"(Image from 2)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Thus, the imperative for efficiency becomes very important. We must explore techniques to mitigate the resource demands of these gargantuan models without sacrificing, or ideally even improving, performance. Two primary avenues present themselves: pruning and quantization. Pruning focuses on reducing the number of parameters, aiming to induce sparsity – a state where a significant proportion of model weights are zero. Quantization, conversely, reduces the precision of the remaining parameters, shrinking their storage footprint. This post will focus on the former: pruning and sparsity in neural networks.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"pruning\",\"children\":[[[\"$\",\"a\",\"link-pruning\",{\"href\":\"#pruning\",\"className\":\"anchor\"}]],\"Pruning\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Neural network pruning, at its core, is about selective weight removal. Formally, we can frame it as a constrained optimization problem:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mo\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"arg\"}],[\"$\",\"mo\",null,{\"children\":\"⁡\"}],[\"$\",\"mi\",null,{\"children\":\"min\"}],[\"$\",\"mo\",null,{\"children\":\"⁡\"}]]}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"W\"}],[\"$\",\"mi\",null,{\"children\":\"p\"}]]}]]}],[\"$\",\"mtext\",null,{\"children\":\" \"}],[\"$\",\"mi\",null,{\"children\":\"L\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"X\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\";\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"W\"}],[\"$\",\"mi\",null,{\"children\":\"p\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mspace\",null,{\"width\":\"1em\"}],[\"$\",\"mtext\",null,{\"children\":\"subject to\"}],[\"$\",\"mspace\",null,{\"width\":\"1em\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∥\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"W\"}],[\"$\",\"mi\",null,{\"children\":\"p\"}]]}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∥\"}],[\"$\",\"mn\",null,{\"children\":\"0\"}]]}],[\"$\",\"mo\",null,{\"children\":\"\u003c\"}],[\"$\",\"mi\",null,{\"children\":\"N\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\mathop{\\\\arg\\\\min}_{W_p} \\\\space L(X; W_p) \\\\quad \\\\text{subject to} \\\\quad \\\\|W_p\\\\|_0 \u003c N\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.1915em\",\"verticalAlign\":\"-0.4415em\"}}],[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[\"ar\",[\"$\",\"span\",null,{\"style\":{\"marginRight\":\"0.01389em\"},\"children\":\"g\"}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":\"min\"}]]}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2342em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.4559em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.13889em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1645em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.357em\",\"marginLeft\":\"-0.1389em\",\"marginRight\":\"0.0714em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.5em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size3 size1 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"p\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2819em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.4415em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"children\":\" \"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"L\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.07847em\"},\"children\":\"X\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\";\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.13889em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1514em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.1389em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"p\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"1em\"}}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"subject to\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"1em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"∥\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.13889em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1514em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.1389em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"p\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"∥\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3011em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"0\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"\u003c\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}]]}]]}]]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Where:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"L\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"L\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"L\"}]]}]}]]}],\" is the loss function guiding network training.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"mathvariant\":\"bold\",\"children\":\"x\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\mathbf{x}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.4444em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathbf\",\"children\":\"x\"}]]}]}]]}],\" represents the input data.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"mathvariant\":\"bold\",\"children\":\"W\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\mathbf{W}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathbf\",\"style\":{\"marginRight\":\"0.01597em\"},\"children\":\"W\"}]]}]}]]}],\" is the original, dense weight matrix.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"bold\",\"children\":\"W\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"bold\",\"children\":\"p\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\mathbf{W_p}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.9722em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathbf\",\"style\":{\"marginRight\":\"0.01597em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1611em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.016em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathbf mtight\",\"children\":\"p\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\" denotes the pruned weight matrix.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∥\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"bold\",\"children\":\"W\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"bold\",\"children\":\"p\"}]]}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∥\"}],[\"$\",\"mn\",null,{\"children\":\"0\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\|\\\\mathbf{W_p}\\\\|_0\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.0361em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"∥\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathbf\",\"style\":{\"marginRight\":\"0.01597em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1611em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.016em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathbf mtight\",\"children\":\"p\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"∥\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3011em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"0\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\" is the \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"ℓ\"}],[\"$\",\"mn\",null,{\"children\":\"0\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\ell_0\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8444em\",\"verticalAlign\":\"-0.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"ℓ\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3011em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"0\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\" \",[\"$\",\"em\",null,{\"children\":\"norm\"}],\", counting the number of non-zero parameters in \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"bold\",\"children\":\"W\"}],[\"$\",\"mi\",null,{\"mathvariant\":\"bold\",\"children\":\"p\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\mathbf{W_p}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.9722em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathbf\",\"style\":{\"marginRight\":\"0.01597em\"},\"children\":\"W\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1611em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.016em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathbf mtight\",\"children\":\"p\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\".\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"N\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"N\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}]]}]}]]}],\" is our target sparsity level - the maximum allowed non-zero weights.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"pruning-strategies-neurons-vs-synapses-and-granularity\",\"children\":[[[\"$\",\"a\",\"link-pruning-strategies-neurons-vs-synapses-and-granularity\",{\"href\":\"#pruning-strategies-neurons-vs-synapses-and-granularity\",\"className\":\"anchor\"}]],\"Pruning Strategies: Neurons vs. Synapses \u0026 Granularity\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Pruning can be broadly categorized by the granularity at which weights are removed. We can prune at the level of individual synapses (connections) or entire neurons (nodes).\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Neuron Pruning\"}],\": This involves eliminating entire neurons, either within a layer (width pruning) or by removing layers altogether (depth pruning). The rationale is that some neurons may be redundant or contribute negligibly to the network's overall function. Removing them simplifies the network architecture, potentially leading to faster inference and reduced memory usage. However, neuron pruning can be a more coarse-grained approach, potentially leading to larger accuracy drops if not performed judiciously.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Synapse Pruning\"}],\": Synapse pruning, conversely, targets individual connections (weights) between neurons. Weights with magnitudes below a certain threshold are set to zero. This induces sparsity at a finer granularity, potentially preserving more of the network's representational capacity compared to aggressive neuron pruning. Synapse pruning can lead to unstructured sparsity (individual weights zeroed out) or structured sparsity (patterns of zeros enforced, as discussed later).\"]}],\"\\n\",\"\\n\",[\"$\",\"img\",null,{\"src\":\"/images/efficiency/2_pruning/2_pruning_neurons_synapses.png\",\"alt\":\"Pruning Neurons vs. Synapses\"}],\"\\n\",[\"$\",\"sub\",null,{\"children\":\"(Image from 3)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Han et al. (2015)\",[\"$\",\"sup\",null,{\"children\":\"3\"}],\" demonstrated the effectiveness of iterative magnitude pruning. They trained a baseline network (AlexNet), then pruned small-magnitude weights, and crucially, fine-tuned the remaining weights. This iterative prune-and-fine-tune cycle proved highly effective.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The Lottery Ticket Hypothesis (Frankle \u0026 Carbin, 2018\",[\"$\",\"sup\",null,{\"children\":\"12\"}],\") posits that within a randomly initialized dense network, there exists a sparse subnetwork (\",[\"$\",\"em\",null,{\"children\":\"winning ticket\"}],\") that can be trained in isolation to achieve comparable or even better performance than the original dense network...(line too long; chars omitted)\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Evaluation Metrics Beyond Accuracy\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"While accuracy preservation is crucial, evaluating pruned models should also consider compression ratio (sparsity level), inference speedup (latency reduction), and energy efficiency gains. Benchmarking across these metrics provides a more holistic picture of pruning effectiveness.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"granularities-of-pruning\",\"children\":[[[\"$\",\"a\",\"link-granularities-of-pruning\",{\"href\":\"#granularities-of-pruning\",\"className\":\"anchor\"}]],\"Granularities of Pruning\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Beyond neuron vs. synapse, pruning can be further categorized by granularity, particularly relevant in Convolutional Neural Networks (CNNs). CNNs, with their kernel-based operations, offer various structural levels for pruning:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Fine-grained (Element-wise) Pruning\"}],\": Individual weights within kernels or weight matrices are pruned based on some criterion (e.g., magnitude). Offers the highest flexibility and potential compression ratio, but often results in unstructured sparsity, which is less hardware-friendly on standard GPUs.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Pattern-based (N:M) Pruning\"}],\": Enforces structured sparsity by requiring specific patterns of zeros within weight blocks (e.g., 2:4 sparsity, where in every block of 4 consecutive values, at least 2 must be zero). This is specifically designed to leverage hardware acceleration, such as NVIDIA's sparse tensor cores, but may limit achievable sparsity compared to fine-grained pruning.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Vector-based (Row/Column) Pruning\"}],\": Prunes entire rows or columns in weight matrices, effectively removing neurons or filters. Strikes a balance between granularity and structure.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Kernel-based Pruning\"}],\": Removes entire kernels in convolutional layers. More aggressive than vector pruning, leading to simpler filters and reduced computation.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Channel-based Pruning\"}],\": Specifically for CNNs, removes entire channels (feature maps). This is often considered more structurally sound for CNNs as channels represent distinct features. Channel pruning can be seen as a form of neuron pruning at the feature map level.\"]}],\"\\n\",\"\\n\",[\"$\",\"img\",null,{\"src\":\"/images/efficiency/2_pruning/6_pruning_granularities.png\",\"alt\":\"Pruning Granularities\"}],\"\\n\",[\"$\",\"sub\",null,{\"children\":\"(Image from 2)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The choice of pruning granularity involves trade-offs between compression ratio, hardware acceleration potential, and implementation complexity. Fine-grained pruning offers maximum flexibility but may not directly map to hardware speedups without specialized sparse compute units. Structured pruning, particularly pattern-based sparsity, is explicitly designed for hardware acceleration but might constrain the achievable sparsity level and require careful implementation.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"pruning-in-industry-nvidias-structured-sparsity\",\"children\":[[[\"$\",\"a\",\"link-pruning-in-industry-nvidias-structured-sparsity\",{\"href\":\"#pruning-in-industry-nvidias-structured-sparsity\",\"className\":\"anchor\"}]],\"Pruning in Industry: NVIDIA's Structured Sparsity\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"NVIDIA's Ampere and Hopper architectures [\",[\"$\",\"sup\",null,{\"children\":\"5\"}],\", \",[\"$\",\"sup\",null,{\"children\":\"6\"}],\"] have brought structured sparsity to the forefront of hardware-accelerated AI inference. Their Tensor Cores, starting from the Ampere generation, leverage 2:4 structured sparsity to achieve up to 2x throughput gains while maintaining accuracy for matrix multiplications.\"]}],\"\\n\",\"\\n\",[\"$\",\"img\",null,{\"src\":\"/images/efficiency/2_pruning/4_nvidia_sparsity_diagram.jpg\",\"alt\":\"NVIDIA Sparsity Diagram\"}],\"\\n\",[\"$\",\"sub\",null,{\"children\":\"(Image from 4)\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"structured-sparsity-24-pattern\",\"children\":[[[\"$\",\"a\",\"link-structured-sparsity-24-pattern\",{\"href\":\"#structured-sparsity-24-pattern\",\"className\":\"anchor\"}]],\"Structured Sparsity: 2:4 Pattern\"]}],\"\\n\",\"\\n\",[\"$\",\"img\",null,{\"src\":\"/images/efficiency/2_pruning/5_structured_sparsity_pattern.png\",\"alt\":\"Structured Sparsity Pattern\"}],\"\\n\",[\"$\",\"sub\",null,{\"children\":\"(Image from 5)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"NVIDIA's sparse Tensor Cores are optimized for 2:4 sparsity. This requires that within every group of four contiguous values in a weight tensor, at least two must be zero. This enforces a 50% sparsity rate. The concept generalizes to N:M sparsity, where in a block of M values, N must be zero. During storage, only the non-zero values and associated metadata (indices indicating their original positions) are kept, leading to compression. This is primarily applicable to fully connected and convolutional layers.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The performance gain arises because Sparse Tensor Cores operate only on the non-zero values. The metadata guides the hardware to fetch only the necessary corresponding values from the dense operand in a matrix multiplication, effectively skipping computations involving zeros \",[\"$\",\"sup\",null,{\"children\":\"6\"}],\". For 2x sparsity, this can, ideally, halve the computation time for matrix multiplications, a core operation in deep learning.\"]}],\"\\n\",\"\\n\",[\"$\",\"img\",null,{\"src\":\"/images/efficiency/2_pruning/7_dense_sparse_matrix_nvidia.png\",\"alt\":\"Dense Sparse Matrix NVIDIA\"}],\"\\n\",[\"$\",\"sub\",null,{\"children\":\"(Image from 7)\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Consider a GEMM (Generalized Matrix Multiply) operation: \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"C\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mi\",null,{\"children\":\"A\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"mi\",null,{\"children\":\"B\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"C = A \\\\times B\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.07153em\"},\"children\":\"C\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7667em\",\"verticalAlign\":\"-0.0833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"A\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.05017em\"},\"children\":\"B\"}]]}]]}]]}],\", where \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"A\"}],[\"$\",\"mo\",null,{\"children\":\"∈\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"double-struck\",\"children\":\"R\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"M\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"mi\",null,{\"children\":\"K\"}]]}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"A \\\\in \\\\mathbb{R}^{M \\\\times K}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7224em\",\"verticalAlign\":\"-0.0391em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"A\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"∈\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8413em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathbb\",\"children\":\"R\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8413em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"M\"}],[\"$\",\"span\",null,{\"className\":\"mbin mtight\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.07153em\"},\"children\":\"K\"}]]}]}]]}]}]}]}]}]]}]]}]]}]]}],\" is sparse (2:4 structured sparsity), and \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"B\"}],[\"$\",\"mo\",null,{\"children\":\"∈\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mi\",null,{\"mathvariant\":\"double-struck\",\"children\":\"R\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"K\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"mi\",null,{\"children\":\"N\"}]]}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"B \\\\in \\\\mathbb{R}^{K \\\\times N}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7224em\",\"verticalAlign\":\"-0.0391em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.05017em\"},\"children\":\"B\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"∈\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8413em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathbb\",\"children\":\"R\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8413em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.07153em\"},\"children\":\"K\"}],[\"$\",\"span\",null,{\"className\":\"mbin mtight\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}]]}]}]]}]}]}]}]}]]}]]}]]}]]}],\" is dense. Sparse Tensor Cores accelerate this by effectively transforming the dense GEMM into a sparse one. While the logical dimensions of \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"A\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"A\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"A\"}]]}]}]]}],\" remain \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"M\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"mi\",null,{\"children\":\"K\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"M \\\\times K\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7667em\",\"verticalAlign\":\"-0.0833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"M\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.07153em\"},\"children\":\"K\"}]]}]]}]]}],\", its compressed representation and the sparse hardware allow computation only on the non-zero elements of \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"A\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"A\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"A\"}]]}]}]]}],\" and the corresponding rows of \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"B\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"B\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.05017em\"},\"children\":\"B\"}]]}]}]]}],\". This hardware-level optimization is a significant driver for adopting structured sparsity in practical deployments.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"However, it's crucial to note that the theoretical 2x speedup is often not fully realized in practice due to overheads like metadata processing and memory access patterns. Empirical benchmarks are essential to quantify the actual gains in specific use cases.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This hardware-level optimization is a significant driver for adopting structured sparsity in practical deployments. However, it's crucial to note that the theoretical 2x speedup is often not fully realized in practice due to overheads like metadata processing and memory access patterns. Empirical benchmarks are essential to quantify the actual gains in specific use cases.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"pruning-criteria\",\"children\":[[[\"$\",\"a\",\"link-pruning-criteria\",{\"href\":\"#pruning-criteria\",\"className\":\"anchor\"}]],\"Pruning Criteria\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The efficacy of pruning relias on identifying and removing \",[\"$\",\"em\",null,{\"children\":\"unimportant\"}],\" weights. Various criteria exist to assess weight importance. Magnitude-based pruning is a common and straightforward approach.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"magnitude-based-pruning\",\"children\":[[[\"$\",\"a\",\"link-magnitude-based-pruning\",{\"href\":\"#magnitude-based-pruning\",\"className\":\"anchor\"}]],\"Magnitude-Based Pruning\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The intuition is that weights with smaller absolute values contribute less to the network's output and can be pruned.\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"element-wise-pruning\",\"children\":[[[\"$\",\"a\",\"link-element-wise-pruning\",{\"href\":\"#element-wise-pruning\",\"className\":\"anchor\"}]],\"Element-wise Pruning\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The simplest form, where individual weights below a threshold are zeroed out.\"}],\"\\n\",\"\\n\",[\"$\",\"img\",null,{\"src\":\"/images/efficiency/2_pruning/8_element_pruning.png\",\"alt\":\"Element Pruning\"}],\"\\n\",[\"$\",\"sub\",null,{\"children\":\"(Image from 2)\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"structural-pruning-with-norms\",\"children\":[[[\"$\",\"a\",\"link-structural-pruning-with-norms\",{\"href\":\"#structural-pruning-with-norms\",\"className\":\"anchor\"}]],\"Structural Pruning with Norms\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"For structured pruning (e.g., row, column, kernel), norms can be used to assess the \",[\"$\",\"em\",null,{\"children\":\"importance\"}],\" of entire structures. Common norms include \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"l\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"l_1\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8444em\",\"verticalAlign\":\"-0.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.01968em\"},\"children\":\"l\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3011em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0197em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"1\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\", \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"l\"}],[\"$\",\"mn\",null,{\"children\":\"2\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"l_2\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8444em\",\"verticalAlign\":\"-0.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.01968em\"},\"children\":\"l\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3011em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0197em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"2\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\", and generalized \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"l\"}],[\"$\",\"mi\",null,{\"children\":\"p\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"l_p\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.9805em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.01968em\"},\"children\":\"l\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1514em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0197em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"p\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\" norms.\"]}],\"\\n\",[\"$\",\"h4\",null,{\"id\":\"object-object-norm\",\"children\":[[[\"$\",\"a\",\"link-object-object-norm\",{\"href\":\"#object-object-norm\",\"className\":\"anchor\"}]],[[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"l\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}]]}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"l_1\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8444em\",\"verticalAlign\":\"-0.15em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.01968em\"},\"children\":\"l\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3011em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0197em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"1\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]}]]}],\"-norm\"]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Importance is calculated as the sum of absolute values within a structural unit (eg: a row):\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mtext\",null,{\"children\":\"Importance\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mo\",null,{\"children\":\"∑\"}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"i\"}],[\"$\",\"mo\",null,{\"children\":\"∈\"}],[\"$\",\"mi\",null,{\"children\":\"S\"}]]}]]}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∣\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"w\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}]]}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"∣\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\text{Importance} = \\\\sum_{i \\\\in S} | w_i | \"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8778em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"Importance\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.0771em\",\"verticalAlign\":\"-0.3271em\"}}],[\"$\",\"span\",null,{\"className\":\"mop\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mop op-symbol small-op\",\"style\":{\"position\":\"relative\",\"top\":\"0em\"},\"children\":\"∑\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.1786em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.4003em\",\"marginLeft\":\"0em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}],[\"$\",\"span\",null,{\"className\":\"mrel mtight\",\"children\":\"∈\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.05764em\"},\"children\":\"S\"}]]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3271em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"∣\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02691em\"},\"children\":\"w\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0269em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"∣\"}]]}]]}]]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"where \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"S\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"S\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.05764em\"},\"children\":\"S\"}]]}]}]]}],\" is the set of indices in the structural units.\"]}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"beyond-magnitude\",\"children\":[[[\"$\",\"a\",\"link-beyond-magnitude\",{\"href\":\"#beyond-magnitude\",\"className\":\"anchor\"}]],\"Beyond Magnitude\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"more-sophisticated-criteria\",\"children\":[[[\"$\",\"a\",\"link-more-sophisticated-criteria\",{\"href\":\"#more-sophisticated-criteria\",\"className\":\"anchor\"}]],\"More Sophisticated Criteria\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"While magnitude-based pruning is widely used due to its simplicity, it's not necessarily the most effective criterion. More advanced methods consider:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Gradient-based Pruning\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Weights with small gradients during training are considered less important. This directly relates to the weight's contribution to loss reduction. Techniques like Optimal Brain Damage (LeCun et al., 1990\",[\"$\",\"sup\",null,{\"children\":\"10\"}],\") and Optimal Brain Surgeon (Hassibi et al., 1993\",[\"$\",\"sup\",null,{\"children\":\"11\"}],\") fall into this category, though they can be computationally expensive for large networks.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Connection Sensitivity\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Methods that attempt to directly estimate the impact of removing a connection on the network's output. This can involve approximating the Hessian of the loss function (second-order information) to assess sensitivity. Again, computationally demanding but potentially more accurate.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Data-Driven/Activation-Based Pruning\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Pruning decisions based on the actual activations observed during inference on a dataset. For instance, neurons or channels with consistently low activations might be deemed less important and pruned.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"pruning-schedules-and-strategies\",\"children\":[[[\"$\",\"a\",\"link-pruning-schedules-and-strategies\",{\"href\":\"#pruning-schedules-and-strategies\",\"className\":\"anchor\"}]],\"Pruning Schedules and Strategies:\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"One-Shot Pruning\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Prune the network once, based on a chosen criterion and sparsity target, then fine-tune the remaining weights. Simple but may be suboptimal for high sparsity levels.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Iterative Pruning\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Repeated cycles of pruning and fine-tuning. As demonstrated by Han et al. 3, this often yields better results, allowing the network to adapt to sparsity more effectively. Schedules can be fixed (prune by X% every Y epochs) or adaptive.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Pruning During Training\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Integrate pruning directly into the training process. Regularization terms can be added to the loss function to encourage sparsity during training itself. Less common than post-training pruning but an active research area.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Dynamic Sparsity\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Sparsity patterns are not fixed but change during training or even during inference. This adds complexity but could potentially offer greater flexibility and efficiency.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"practical-considerations-and-open-questions\",\"children\":[[[\"$\",\"a\",\"link-practical-considerations-and-open-questions\",{\"href\":\"#practical-considerations-and-open-questions\",\"className\":\"anchor\"}]],\"Practical Considerations \u0026 Open Questions\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"While pruning offers a promising route to AI efficiency, several practical aspects and open research questions remain:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Software Tools and Libraries\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Frameworks like PyTorch and TensorFlow provide built-in modules and libraries to facilitate pruning. NVIDIA TensorRT 4 is crucial for deploying sparse models on NVIDIA hardware, optimizing for sparse tensor core utilization. However, tooling is still evolving, and seamless integration of complex pruning strategies can be challenging.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Hyperparameter Tuning for Pruning\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Pruning introduces new hyperparameters: sparsity level, pruning schedule, fine-tuning epochs, pruning criterion thresholds, etc. Tuning these effectively can be non-trivial and often requires experimentation. Automated pruning hyperparameter search is an area of active research.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Model Architectures and Pruning Sensitivity\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Different network architectures exhibit varying degrees of prune-friendliness. Some architectures are inherently more robust to pruning than others. Understanding architectural biases towards sparsity is crucial for effective pruning. Transformers, for instance, may respond differently to pruning compared to CNNs.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Impact on Training Time\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"While the goal is to reduce inference time, the pruning process itself (especially iterative pruning with fine-tuning) can add to the overall training time. Careful scheduling and efficient implementation are needed to mitigate this.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Quantization and Pruning Synergy\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Pruning and quantization are often used in tandem. Applying quantization to already sparse models can further compress them and improve inference efficiency. Exploring the optimal combination and ordering of these techniques is an ongoing area of investigation.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Pruning Large Language Models (LLMs)\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Pruning LLMs presents unique challenges due to their scale and the sensitivity of their performance to even small perturbations. Maintaining perplexity and generation quality after pruning is paramount. Techniques like attention head pruning, layer pruning, and specialized sparse training methods are actively being explored for LLMs.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Lottery Ticket Hypothesis\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The Lottery Ticket Hypothesis (Frankle \u0026 Carbin, 2018\",[\"$\",\"sup\",null,{\"children\":\"12\"}],\") posits that within a randomly initialized dense network, there exists a sparse subnetwork (\",[\"$\",\"em\",null,{\"children\":\"winning ticket\"}],\") that can be trained in isolation to achieve comparable or even better performance than the original dense network. This suggests that sparsity is not just about compression but might be fundamental to efficient learning itself. Exploring lottery tickets and related sparse initialization strategies is a promising research direction.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Evaluation Metrics Beyond Accuracy\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"While accuracy preservation is crucial, evaluating pruned models should also consider compression ratio (sparsity level), inference speedup (latency reduction), and energy efficiency gains. Benchmarking across these metrics provides a more holistic picture of pruning effectiveness.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"future-directions\",\"children\":[[[\"$\",\"a\",\"link-future-directions\",{\"href\":\"#future-directions\",\"className\":\"anchor\"}]],\"Future Directions\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Research continues to push the boundaries of pruning. Areas of focus include developing more robust and automated pruning criteria, exploring dynamic sparsity, designing hardware-aware pruning algorithms, and understanding the theoretical underpinnings of sparsity in deep learning.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"conclusion-sparse-futures\",\"children\":[[[\"$\",\"a\",\"link-conclusion-sparse-futures\",{\"href\":\"#conclusion-sparse-futures\",\"className\":\"anchor\"}]],\"Conclusion: Sparse Futures\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Pruning and sparsity offer a critical pathway to reconcile the insatiable demands of modern AI with the constraints of computational resources and energy efficiency. While challenges remain in terms of tooling, hyperparameter optimization, and achieving consistent and predictable performance gains, the potential benefits are undeniable. As hardware support for sparsity matures and research deepens our understanding of sparse neural networks, pruning is poised to become an indispensable technique in the toolkit of any AI practitioner aiming for both performance and sustainability. The pursuit of sparsity is not just about making models smaller; it's about sculpting more efficient and elegant intelligence.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"acknowledgements\",\"children\":[[[\"$\",\"a\",\"link-acknowledgements\",{\"href\":\"#acknowledgements\",\"className\":\"anchor\"}]],\"Acknowledgements\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"This blog series on AI efficiency is heavily indebted to Dr. Song Han's course on TinyML and Efficient Deep Learning Computing\",[\"$\",\"sup\",null,{\"children\":\"8\"}],\". The references cited and the broader field of efficient deep learning are built upon the work of numerous researchers and engineers.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"I would like to extend my sincere gratitude to my colleagues for their invaluable insights and assistance throughout this research: Grok-2.0 \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://x.ai/blog/grok-2\",\"children\":\"(xAI)\"}],\",\\nGemini-2.0-Flash-Thinking-Exp-01-21 \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://deepmind.google/technologies/gemini/flash-thinking/\",\"children\":\"(DeepMind)\"}],\", O1 \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://openai.com/o1/\",\"children\":\"(OpenAI)\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Their contributions have been pivotal to the success of this project, and I am deeply thankful for their collaboration.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"references\",\"children\":[[[\"$\",\"a\",\"link-references\",{\"href\":\"#references\",\"className\":\"anchor\"}]],\"References\"]}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"M. Horowitz, \\\"1.1 Computing's energy problem (and what we can do about it),\\\" 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), San Francisco, CA, USA, 2014, pp. 10-14, doi: 10.1109/ISSCC.2014.6757323. https://ieeexplore.ieee.org/document/6757323\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"EfficientML.ai Lecture 3 - Pruning and Sparsity Part I (MIT 6.5940, Fall 2024). https://www.youtube.com/watch?v=EjsB0WgIfUM\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Han, S., Mao, H., \u0026 Dally, W. J. (2015). Learning both Weights and Connections for Efficient Neural Networks. Advances in Neural Information Processing Systems, 28. https://arxiv.org/abs/1506.02626\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"How Sparsity Adds Umph to AI Inference. https://blogs.nvidia.com/blog/sparsity-ai-inference/\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Mishra, A., Albericio Latorre, J., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., \u0026 Micikevicius, P. (2021). \",[\"$\",\"em\",null,{\"children\":\"Accelerating sparse deep neural networks\"}],\" (arXiv:2104.08378 [cs.LG]). arXiv. https://arxiv.org/abs/2104.08378\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Dr. Song Han's course on TinyML and Efficient Deep Learning Computing. https://hanlab.mit.edu/courses/2024-fall-65940\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Branwen, G. (2020). The Scaling Hypothesis. https://gwern.net/scaling-hypothesis\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"LeCun, Y., Denker, J., \u0026 Solla, S. (1989). Optimal brain damage. In D. Touretzky (Ed.), Advances in Neural Information Processing Systems (Vol. 2, pp. ). Morgan-Kaufmann. https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Hassibi, B., \u0026 Stork, D. (1992). Second order derivatives for network pruning: Optimal Brain Surgeon. In S. Hanson, J. Cowan, \u0026 C. Giles (Eds.), Advances in Neural Information Processing Systems (Vol. 5, pp. ). Morgan-Kaufmann. https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Frankle, J., \u0026 Carbin, M. (2019). The lottery ticket hypothesis: Finding sparse, trainable neural networks (arXiv:1803.03635 [cs.LG]). arXiv. https://arxiv.org/abs/1803.03635\"}],\"\\n\"]}]]\n"])</script></body></html>
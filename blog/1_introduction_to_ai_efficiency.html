<!DOCTYPE html><html lang="en" class="text-black bg-white dark:text-white dark:bg-black __variable_ac79ff __variable_8a4d12"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/ec1a1eae803b668e-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/f980ec13b5b5e554.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/images/efficiency/1_introduction/scaling-hypothesis.png"/><link rel="preload" as="image" href="/images/efficiency/1_introduction/H200-datasheet.png"/><link rel="stylesheet" href="/_next/static/css/6b33b3fa6fbfebf3.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/09dfadb69bdaa005.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-d43983cff62b5160.js"/><script src="/_next/static/chunks/eae8abb3-fefb6374c2cd47f6.js" async=""></script><script src="/_next/static/chunks/656-9d4786b79d33cc4a.js" async=""></script><script src="/_next/static/chunks/main-app-dbbdcec500ee1fe5.js" async=""></script><script src="/_next/static/chunks/798-0bc533209101da7e.js" async=""></script><script src="/_next/static/chunks/241-2e7d2ff137352cd4.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-452d005f2d15aea3.js" async=""></script><script src="/_next/static/chunks/app/layout-5ee8c40b1c377a89.js" async=""></script><title>AI Efficiency I: An introduction | ekdnam</title><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><meta property="og:title" content="AI Efficiency I: An introduction"/><meta property="og:url" content="https://portfolio-blog-starter.vercel.app/blog/1_introduction_to_ai_efficiency"/><meta property="og:image" content="https://portfolio-blog-starter.vercel.app/og?title=AI%20Efficiency%20I%3A%20An%20introduction"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2024-01-10"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="AI Efficiency I: An introduction"/><meta name="twitter:image" content="https://portfolio-blog-starter.vercel.app/og?title=AI%20Efficiency%20I%3A%20An%20introduction"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="antialiased max-w-xl mx-4 mt-8 lg:mx-auto"><main class="flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0"><aside class="-ml-[8px] mb-16 tracking-tight"><div class="lg:sticky lg:top-20"><nav class="flex flex-row items-start relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative" id="nav"><div class="flex flex-row space-x-0 pr-10"><a class="transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1" href="/">home</a><a class="transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1" href="/blog">blog</a></div></nav></div></aside><section><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"AI Efficiency I: An introduction","datePublished":"2024-01-10","dateModified":"2024-01-10","image":"/og?title=AI%20Efficiency%20I%3A%20An%20introduction","url":"https://portfolio-blog-starter.vercel.app/blog/1_introduction_to_ai_efficiency","author":{"@type":"Person","name":"My Portfolio"}}</script><h1 class="title font-semibold text-2xl tracking-tighter">AI Efficiency I: An introduction</h1><div class="flex justify-between items-center mt-2 mb-8 text-sm"><p class="text-sm text-neutral-600 dark:text-neutral-400">January 10, 2024</p></div><article class="prose"><h1 id="introduction"><a href="#introduction" class="anchor"></a>Introduction</h1>
<p>In recent years, large language models (LLMs) have shown remarkable abilities—writing coherent text, coding, analyzing images—yet their resource requirements grow alarmingly. This leads to cost barriers, environmental concerns, and engineering hurdles. We face a reckoning: while scaling up parameters and data yields performance gains, simply throwing more GPUs and memory at these problems becomes unsustainable. Hardware constraints, energy consumption, and deployment demands force us to seek new strategies for optimization.</p>
<p>Indeed, beyond just bigger models, researchers have begun exploring alternatives: compressing model representations (quantization, pruning, knowledge distillation), tailoring architectures (transformers, neural architecture search), and innovating in distributed training. Each of these areas contributes to a broader theme—efficiency. Rather than solely scaling, we must also refine. This first post will establish some essential context, so we can later examine specialized techniques that reduce cost, time, and environmental impact while retaining model quality.</p>
<hr/>
<h1 id="the-scaling-hypothesis"><a href="#the-scaling-hypothesis" class="anchor"></a>The Scaling Hypothesis</h1>
<p>In large language models (LLMs), the <a target="_blank" rel="noopener noreferrer" href="https://gwern.net/scaling-hypothesis">scaling hypothesis</a> has proven crucial: train bigger neural networks with more data, and ever more sophisticated behaviors emerge. This parallels the evolutionary scaling from primate to human brains. The earliest detailed statement of this emerged in OpenAI’s “<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/pdf/2001.08361">Scaling Laws for Neural Language Models</a>” (2020), which noted that test loss follows a power-law in model size, dataset size, and compute. In plain terms, larger models (with more parameters) plus larger datasets plus more compute leads to lower loss—and hence better performance.</p>
<p>This drives the worldwide LLM arms race, catapults NVIDIA’s valuation, and influences how many billions of parameters “Llama” releases (7B, 14B, 70B, 405B, etc.). I will explore the why and how of this in subsequent posts.</p>
<!-- -->
<img src="/images/efficiency/1_introduction/scaling-hypothesis.png" alt="Scaling Hypothesis"/>
<p>Consider a model with (N) parameters trained on a dataset of size (D), requiring compute (C). Scaling them all up in tandem decreases test loss. In practice, we can sometimes predict performance given partial knowledge of (N), (D), or (C). Researchers exploit this to calibrate how large a model to train if you have limited GPU capacity, or to estimate final performance if you push (N) as high as your hardware budget allows.</p>
<p>Hence, if Llama-405B outperforms Llama-70B, it’s precisely because we have more parameters—provided we can feed them enough data.</p>
<hr/>
<h1 id="model-precision"><a href="#model-precision" class="anchor"></a>Model Precision</h1>
<p>Parameters can be stored at different floating-point precisions:</p>
<ol>
<li><strong>FP32</strong> (32-bit floats)</li>
<li><strong>FP16</strong> (16-bit floats)</li>
<li><strong>FP8</strong> (8-bit floats)</li>
</ol>
<p>Higher-bit precision requires more memory, which matters at large scale. For instance, a 405B-parameter model in <strong>FP32</strong> uses about:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>405</mn><mo>×</mo><msup><mn>10</mn><mn>9</mn></msup><mtext> parameters</mtext><mo>×</mo><mn>4</mn><mtext> bytes/parameter</mtext><mo>=</mo><mn>1.62</mn><mo>×</mo><msup><mn>10</mn><mn>12</mn></msup><mtext> bytes</mtext><mo>≈</mo><mn>1.6</mn><mtext> TB</mtext></mrow><annotation encoding="application/x-tex">405 \times 10^9 \text{ parameters} \times 4 \text{ bytes/parameter} = 1.62 \times 10^{12} \text{ bytes} \approx 1.6 \text{ TB}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">405</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span><span class="mord text"><span class="mord"> parameters</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">4</span><span class="mord text"><span class="mord"> bytes/parameter</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.62</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">12</span></span></span></span></span></span></span></span></span><span class="mord text"><span class="mord"> bytes</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">1.6</span><span class="mord text"><span class="mord"> TB</span></span></span></span></span></p>
<p>That is obviously unwieldy. Real-world deployments typically use more compact representations (e.g., FP16, or even 8-bit quantization).</p>
<hr/>
<h1 id="ai-accelerators"><a href="#ai-accelerators" class="anchor"></a>AI Accelerators</h1>
<p>When training neural networks, we do a forward pass (to compute outputs and loss) and a backward pass (to update parameters). These passes are computation-heavy, requiring trillions of floating-point operations. GPT-3’s training, for example, reportedly took on the order of (3.14\times10^<!-- -->23<!-- -->) FLOPs, implying tremendous hardware and time demands if done naively.</p>
<p>To train or serve such a model efficiently, three main hardware bottlenecks arise:</p>
<ol>
<li><strong>Compute</strong>: speed of forward and backward passes</li>
<li><strong>Memory bandwidth</strong>: rate of loading/storing model parameters</li>
<li><strong>Memory size</strong>: capacity of the hardware accelerator</li>
</ol>
<p>Being first to launch an LLM can secure market share, so reducing training times is vital. Enter the modern AI accelerators, with specialized architectures to handle these tasks swiftly.</p>
<hr/>
<h2 id="introduction-to-gpus"><a href="#introduction-to-gpus" class="anchor"></a>Introduction to GPUs</h2>
<p>GPUs were originally for rendering graphics. Over the years, they’ve grown into the staple for massive parallelism needed in machine learning, scientific simulations, and cryptography. Key GPU design elements include:</p>
<ul>
<li><strong>High Bandwidth Memory (HBM)</strong></li>
<li><strong>Tensor Cores</strong></li>
</ul>
<h3 id="high-bandwidth-memory-hbm"><a href="#high-bandwidth-memory-hbm" class="anchor"></a>High Bandwidth Memory (HBM)</h3>
<p>HBM stacks memory vertically on the same package as the GPU, offering thousands of I/O pins. This yields far higher bandwidth—often terabytes per second—than standard GDDR. In deep learning, models constantly shuffle data to/from memory, so memory bandwidth can become the limiting factor. HBM helps ensure large models are not starved of data.</p>
<h3 id="tensor-cores"><a href="#tensor-cores" class="anchor"></a>Tensor Cores</h3>
<p>Modern GPUs add <strong>Tensor Cores</strong>, which accelerate matrix multiplications for deep learning. By using <strong>mixed-precision arithmetic</strong> (e.g., FP16 multiply with FP32 accumulate), Tensor Cores handle large matrix ops in a single operation. This massively boosts throughput on training and inference tasks—provided the memory system is fast enough to keep them busy.</p>
<h3 id="why-memory-bandwidth-matters"><a href="#why-memory-bandwidth-matters" class="anchor"></a>Why Memory Bandwidth Matters</h3>
<p>Tensor Cores can do trillions of operations per second. If the memory subsystem can’t feed them data at a comparable rate, the GPU stalls. Hence, effective GPU design balances high compute with equally high memory bandwidth.</p>
<h2 id="the-software-angle"><a href="#the-software-angle" class="anchor"></a>The Software Angle</h2>
<p>Alongside hardware improvements, software frameworks like PyTorch, TensorFlow, and JAX heavily influence training speed. These frameworks integrate with HPC libraries that manage GPU kernels, scheduling, and data transfer. The choice of distributed training strategies—model parallel, data parallel, or pipeline parallel—further impacts resource utilization. Even the best hardware can underperform if the software stack is not optimized for parallel operations and memory management.</p>
<hr/>
<h1 id="nvidia-h200-example"><a href="#nvidia-h200-example" class="anchor"></a>NVIDIA H200 Example</h1>
<p>Let’s briefly consider the new NVIDIA H200 (<a target="_blank" rel="noopener noreferrer" href="https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446">datasheet</a>).</p>
<!-- -->
<img src="/images/efficiency/1_introduction/H200-datasheet.png" alt="H200 Datasheet"/>
<h2 id="compute"><a href="#compute" class="anchor"></a>Compute</h2>
<ul>
<li><strong>FP32 throughput</strong>: ~989 TFLOPs. If you tried to train GPT-3 (~3.14e23 FLOPs total) on one H200 at full FP32 speed, you’d need on the order of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3.14</mn><mo>×</mo><msup><mn>10</mn><mn>23</mn></msup><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mn>989</mn><mo>×</mo><msup><mn>10</mn><mn>12</mn></msup><mo stretchy="false">)</mo><mo>≈</mo><mn>3.17</mn><mo>×</mo><msup><mn>10</mn><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">3.14\times10^{23} / (989\times10^{12}) \approx 3.17\times10^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">3.14</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">23</span></span></span></span></span></span></span></span></span><span class="mord">/</span><span class="mopen">(</span><span class="mord">989</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">12</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">3.17</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span> seconds (~10 years).</li>
<li><strong>FP8 Tensor Cores</strong>: ~3958 TFLOPs. This quadruples raw throughput, reducing that naive 10-year figure to ~2.5 years—still impractical, but illustrative of what lower precision can do.</li>
</ul>
<h2 id="memory-size"><a href="#memory-size" class="anchor"></a>Memory (Size)</h2>
<ul>
<li>The H200 has <strong>141GB</strong> of memory. GPT-3 in FP32 alone occupies ~700GB, so you’d need multiple GPUs just to hold the entire model in memory.</li>
</ul>
<h2 id="memory-bandwidth"><a href="#memory-bandwidth" class="anchor"></a>Memory (Bandwidth)</h2>
<ul>
<li>The H200’s memory bandwidth is <strong>4.8TB/s</strong>. Even if you have enough GPUs in parallel, training speed depends heavily on how fast you can transfer data.</li>
</ul>
<hr/>
<h1 id="time-to-train-simplified"><a href="#time-to-train-simplified" class="anchor"></a>Time to Train, Simplified</h1>
<p>If one naive approach divides the model and data across (n) GPUs, each GPU’s training time might look like:</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Time</mtext><mo>∝</mo><mi>n</mi><mo>×</mo><mo stretchy="false">(</mo><mtext>compute time</mtext><mo>+</mo><mn>2</mn><mo>×</mo><mtext>memory-load time</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Time} \propto n \times (\text{compute time} + 2 \times \text{memory-load time})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord text"><span class="mord">Time</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord text"><span class="mord">compute time</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">memory-load time</span></span><span class="mclose">)</span></span></span></span></p>
<p>(The factor of 2 is for loading and unloading data chunks.) In practice, you use <strong>model parallelism</strong> and <strong>collective communication</strong> to share gradients, so it’s more complicated. But the takeaway is:</p>
<ul>
<li>More GPUs <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext></mrow><annotation encoding="application/x-tex">\implies</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.549em;vertical-align:-0.024em"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">⟹</span><span class="mspace" style="margin-right:0.2778em"></span></span></span></span> more memory and more overall TFLOPs</li>
<li>Lower precision <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext></mrow><annotation encoding="application/x-tex">\implies</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.549em;vertical-align:-0.024em"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">⟹</span><span class="mspace" style="margin-right:0.2778em"></span></span></span></span>higher effective FLOPs</li>
<li>Higher bandwidth <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext></mrow><annotation encoding="application/x-tex">\implies</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.549em;vertical-align:-0.024em"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">⟹</span><span class="mspace" style="margin-right:0.2778em"></span></span></span></span> less stalling</li>
</ul>
<p>Thus, training can be shortened, at the cost of hardware expense and engineering complexity.</p>
<hr/>
<h1 id="distributed-hpc-clusters"><a href="#distributed-hpc-clusters" class="anchor"></a>Distributed HPC Clusters</h1>
<p>Beyond a single GPU or even a single node, large-scale AI systems often run on multi-node high-performance computing (HPC) clusters. These clusters orchestrate many GPUs in parallel to reduce total training time. However, scaling to dozens or hundreds of GPUs introduces communication overheads—each GPU must exchange model gradients and parameters, typically using specialized libraries (e.g., NVIDIA NCCL).</p>
<p>As model sizes grow, the percentage of time GPUs spend simply synchronizing data can climb. Efficient collective operations, high-bandwidth interconnects like NVLink, InfiniBand, or 400Gb Ethernet, and custom topologies all help alleviate bottlenecks. Still, practitioners often find that real-world efficiency lags behind raw theoretical FLOPs, underscoring that “adding more GPUs” is no panacea without carefully managing data flow across the entire cluster.</p>
<hr/>
<h1 id="why-we-need-efficiency"><a href="#why-we-need-efficiency" class="anchor"></a>Why We Need Efficiency</h1>
<p>Running enormous models is neither cheap nor straightforward. Data centers pull megawatts of power, GPUs cost tens of thousands of dollars each, and the carbon footprints of large-scale training runs grow worrying. Researchers and enterprises want to optimize for minimal cost, faster turnaround, and smaller carbon impact without sacrificing model quality. Efficiency helps ensure that big ideas remain attainable—and not just for a handful of companies with colossal budgets.</p>
<p>We can’t simply turn the “scale knob” forever. Efficient methods—whether they be hardware-based (quantization, specialized memory) or algorithmic (pruning, better architectures, distributed training paradigms)—let us capture the benefits of scale while mitigating the downsides. It’s not just about racing to the largest parameter count; it’s about making the largest count feasible.</p>
<h2 id="further-environmental-and-social-implications"><a href="#further-environmental-and-social-implications" class="anchor"></a>Further Environmental and Social Implications</h2>
<p>Massive AI training runs demand correspondingly massive energy inputs, straining power grids and increasing global carbon emissions. Some estimates suggest state-of-the-art models can produce emissions on par with entire vehicle lifecycles. These considerations motivate new research not just in hardware efficiency—like better memory systems and specialized circuits—but also in algorithmic techniques that can drastically cut training iterations. Efficiency, therefore, is about more than saving money or time. It aligns AI development with sustainability goals and can open the door for smaller organizations to contribute cutting-edge innovations without incurring prohibitive resource demands.</p>
<hr/>
<h1 id="conclusion"><a href="#conclusion" class="anchor"></a>Conclusion</h1>
<p>This post spotlighted the critical pillars of AI efficiency: how bigger LLMs drive performance but impose severe hardware and cost constraints. We surveyed GPUs, memory bandwidth, and the interplay of precision, parameter count, and data size. While brute-force scaling can work, it collides with practical limits—training times stretch to years, hardware budgets skyrocket, and model footprints become colossal.</p>
<p>In subsequent posts, I will outline the broad strategies that aim to balance performance and pragmatism: pruning or sparsifying large models, automating architecture searches for optimal efficiency, quantizing parameters, distilling knowledge into smaller nets, refining transformers to handle longer contexts, and deploying LLMs at scale across distributed clusters. Each technique represents a piece of the puzzle: how do we keep pushing AI forward without exponentially inflating resource use?</p>
<p>By exploring these methods, we can uncover ways to sustain the breakneck pace of AI progress without abandoning reason—or our electric grid—in the process.</p>
<h1 id="tags"><a href="#tags" class="anchor"></a>Tags</h1>
<p>#AI #efficiency #hardware #gpu</p></article></section><footer class="mb-16"><ul class="font-sm mt-8 flex flex-col space-x-0 space-y-2 text-neutral-600 md:flex-row md:space-x-4 md:space-y-0 dark:text-neutral-300"><li><a class="flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100" rel="noopener noreferrer" target="_blank" href="https://x.com/ekdnam"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">𝕏</p></a></li><li><a class="flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100" rel="noopener noreferrer" target="_blank" href="https://github.com/ekdnam"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">github</p></a></li><li><a class="flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100" rel="noopener noreferrer" target="_blank" href="https://linkedin.com/in/adityamandke"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">linkedin</p></a></li></ul><p class="mt-8 text-neutral-600 dark:text-neutral-300">© <!-- -->2025<!-- --> MIT Licensed</p></footer><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></main><script src="/_next/static/chunks/webpack-d43983cff62b5160.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/ec1a1eae803b668e-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/f980ec13b5b5e554.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/css/6b33b3fa6fbfebf3.css\",\"style\"]\n4:HL[\"/_next/static/css/09dfadb69bdaa005.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"5:I[6373,[],\"\"]\n8:I[2146,[],\"\"]\na:I[3545,[],\"\"]\nb:I[6798,[\"798\",\"static/chunks/798-0bc533209101da7e.js\",\"241\",\"static/chunks/241-2e7d2ff137352cd4.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-452d005f2d15aea3.js\"],\"\"]\nc:I[206,[\"798\",\"static/chunks/798-0bc533209101da7e.js\",\"185\",\"static/chunks/app/layout-5ee8c40b1c377a89.js\"],\"Analytics\"]\nd:I[1289,[\"798\",\"static/chunks/798-0bc533209101da7e.js\",\"185\",\"static/chunks/app/layout-5ee8c40b1c377a89.js\"],\"SpeedInsights\"]\nf:I[55,[],\"\"]\n9:[\"slug\",\"1_introduction_to_ai_efficiency\",\"d\"]\n10:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L5\",null,{\"buildId\":\"LqnAkREHXUbULLy5EfZKv\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"blog\",\"1_introduction_to_ai_efficiency\"],\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"1_introduction_to_ai_efficiency\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"1_introduction_to_ai_efficiency\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"1_introduction_to_ai_efficiency\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L6\",\"$L7\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/09dfadb69bdaa005.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]],null],null]},[null,[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/6b33b3fa6fbfebf3.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"text-black bg-white dark:text-white dark:bg-black __variable_ac79ff __variable_8a4d12\",\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased max-w-xl mx-4 mt-8 lg:mx-auto\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"-ml-[8px] mb-16 tracking-tight\",\"children\":[\"$\",\"div\",null,{\"className\":\"lg:sticky lg:top-20\",\"children\":[\"$\",\"nav\",null,{\"className\":\"flex flex-row items-start relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative\",\"id\":\"nav\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-row space-x-0 pr-10\",\"children\":[[\"$\",\"$Lb\",\"/\",{\"href\":\"/\",\"className\":\"transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1\",\"children\":\"home\"}],[\"$\",\"$Lb\",\"/blog\",{\"href\":\"/blog\",\"className\":\"transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1\",\"children\":\"blog\"}]]}]}]}]}],[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"section\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-8 text-2xl font-semibold tracking-tighter\",\"children\":\"404 - Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":\"The page you are looking for does not exist.\"}]]}],\"notFoundStyles\":[]}],[\"$\",\"footer\",null,{\"className\":\"mb-16\",\"children\":[[\"$\",\"ul\",null,{\"className\":\"font-sm mt-8 flex flex-col space-x-0 space-y-2 text-neutral-600 md:flex-row md:space-x-4 md:space-y-0 dark:text-neutral-300\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"https://x.com/ekdnam\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"𝕏\"}]]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"https://github.com/ekdnam\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"github\"}]]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800 dark:hover:text-neutral-100\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"https://linkedin.com/in/adityamandke\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"linkedin\"}]]}]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-neutral-600 dark:text-neutral-300\",\"children\":[\"© \",2025,\" MIT Licensed\"]}]]}],[\"$\",\"$Lc\",null,{}],[\"$\",\"$Ld\",null,{}]]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Le\"],\"globalErrorComponent\":\"$f\",\"missingSlots\":\"$W10\"}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"section\",null,{\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"suppressHydrationWarning\":true,\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"AI Efficiency I: An introduction\\\",\\\"datePublished\\\":\\\"2024-01-10\\\",\\\"dateModified\\\":\\\"2024-01-10\\\",\\\"image\\\":\\\"/og?title=AI%20Efficiency%20I%3A%20An%20introduction\\\",\\\"url\\\":\\\"https://portfolio-blog-starter.vercel.app/blog/1_introduction_to_ai_efficiency\\\",\\\"author\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"My Portfolio\\\"}}\"}}],[\"$\",\"h1\",null,{\"className\":\"title font-semibold text-2xl tracking-tighter\",\"children\":\"AI Efficiency I: An introduction\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center mt-2 mb-8 text-sm\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-sm text-neutral-600 dark:text-neutral-400\",\"children\":\"January 10, 2024\"}]}],[\"$\",\"article\",null,{\"className\":\"prose\",\"children\":\"$L11\"}]]}]\ne:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"AI Efficiency I: An introduction | ekdnam\"}],[\"$\",\"meta\",\"3\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"4\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"AI Efficiency I: An introduction\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://portfolio-blog-starter.vercel.app/blog/1_introduction_to_ai_efficiency\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image\",\"content\":\"https://portfolio-blog-starter.vercel.app/og?title=AI%20Efficiency%20I%3A%20An%20introduction\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"9\",{\"property\":\"article:published_time\",\"content\":\"2024-01-10\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"AI Efficiency I: An introduction\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:image\",\"content\":"])</script><script>self.__next_f.push([1,"\"https://portfolio-blog-starter.vercel.app/og?title=AI%20Efficiency%20I%3A%20An%20introduction\"}],[\"$\",\"meta\",\"13\",{\"name\":\"next-size-adjust\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,"11:[[\"$\",\"h1\",null,{\"id\":\"introduction\",\"children\":[[[\"$\",\"a\",\"link-introduction\",{\"href\":\"#introduction\",\"className\":\"anchor\"}]],\"Introduction\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In recent years, large language models (LLMs) have shown remarkable abilities—writing coherent text, coding, analyzing images—yet their resource requirements grow alarmingly. This leads to cost barriers, environmental concerns, and engineering hurdles. We face a reckoning: while scaling up parameters and data yields performance gains, simply throwing more GPUs and memory at these problems becomes unsustainable. Hardware constraints, energy consumption, and deployment demands force us to seek new strategies for optimization.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Indeed, beyond just bigger models, researchers have begun exploring alternatives: compressing model representations (quantization, pruning, knowledge distillation), tailoring architectures (transformers, neural architecture search), and innovating in distributed training. Each of these areas contributes to a broader theme—efficiency. Rather than solely scaling, we must also refine. This first post will establish some essential context, so we can later examine specialized techniques that reduce cost, time, and environmental impact while retaining model quality.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"the-scaling-hypothesis\",\"children\":[[[\"$\",\"a\",\"link-the-scaling-hypothesis\",{\"href\":\"#the-scaling-hypothesis\",\"className\":\"anchor\"}]],\"The Scaling Hypothesis\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In large language models (LLMs), the \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://gwern.net/scaling-hypothesis\",\"children\":\"scaling hypothesis\"}],\" has proven crucial: train bigger neural networks with more data, and ever more sophisticated behaviors emerge. This parallels the evolutionary scaling from primate to human brains. The earliest detailed statement of this emerged in OpenAI’s “\",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://arxiv.org/pdf/2001.08361\",\"children\":\"Scaling Laws for Neural Language Models\"}],\"” (2020), which noted that test loss follows a power-law in model size, dataset size, and compute. In plain terms, larger models (with more parameters) plus larger datasets plus more compute leads to lower loss—and hence better performance.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This drives the worldwide LLM arms race, catapults NVIDIA’s valuation, and influences how many billions of parameters “Llama” releases (7B, 14B, 70B, 405B, etc.). I will explore the why and how of this in subsequent posts.\"}],\"\\n\",\"\\n\",[\"$\",\"img\",null,{\"src\":\"/images/efficiency/1_introduction/scaling-hypothesis.png\",\"alt\":\"Scaling Hypothesis\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Consider a model with (N) parameters trained on a dataset of size (D), requiring compute (C). Scaling them all up in tandem decreases test loss. In practice, we can sometimes predict performance given partial knowledge of (N), (D), or (C). Researchers exploit this to calibrate how large a model to train if you have limited GPU capacity, or to estimate final performance if you push (N) as high as your hardware budget allows.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Hence, if Llama-405B outperforms Llama-70B, it’s precisely because we have more parameters—provided we can feed them enough data.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"model-precision\",\"children\":[[[\"$\",\"a\",\"link-model-precision\",{\"href\":\"#model-precision\",\"className\":\"anchor\"}]],\"Model Precision\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Parameters can be stored at different floating-point precisions:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"FP32\"}],\" (32-bit floats)\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"FP16\"}],\" (16-bit floats)\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"FP8\"}],\" (8-bit floats)\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Higher-bit precision requires more memory, which matters at large scale. For instance, a 405B-parameter model in \",[\"$\",\"strong\",null,{\"children\":\"FP32\"}],\" uses about:\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"405\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"10\"}],[\"$\",\"mn\",null,{\"children\":\"9\"}]]}],[\"$\",\"mtext\",null,{\"children\":\" parameters\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"mn\",null,{\"children\":\"4\"}],[\"$\",\"mtext\",null,{\"children\":\" bytes/parameter\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mn\",null,{\"children\":\"1.62\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"10\"}],[\"$\",\"mn\",null,{\"children\":\"12\"}]]}],[\"$\",\"mtext\",null,{\"children\":\" bytes\"}],[\"$\",\"mo\",null,{\"children\":\"≈\"}],[\"$\",\"mn\",null,{\"children\":\"1.6\"}],[\"$\",\"mtext\",null,{\"children\":\" TB\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"405 \\\\times 10^9 \\\\text{ parameters} \\\\times 4 \\\\text{ bytes/parameter} = 1.62 \\\\times 10^{12} \\\\text{ bytes} \\\\approx 1.6 \\\\text{ TB}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7278em\",\"verticalAlign\":\"-0.0833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"405\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.0085em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"0\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8141em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"9\"}]}]]}]}]}]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\" parameters\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"4\"}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\" bytes/parameter\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7278em\",\"verticalAlign\":\"-0.0833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1.62\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.0085em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"0\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8141em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"12\"}]}]}]]}]}]}]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\" bytes\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"≈\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1.6\"}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\" TB\"}]}]]}]]}]]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"That is obviously unwieldy. Real-world deployments typically use more compact representations (e.g., FP16, or even 8-bit quantization).\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"ai-accelerators\",\"children\":[[[\"$\",\"a\",\"link-ai-accelerators\",{\"href\":\"#ai-accelerators\",\"className\":\"anchor\"}]],\"AI Accelerators\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"When training neural networks, we do a forward pass (to compute outputs and loss) and a backward pass (to update parameters). These passes are computation-heavy, requiring trillions of floating-point operations. GPT-3’s training, for example, reportedly took on the order of (3.14\\\\times10^\",23,\") FLOPs, implying tremendous hardware and time demands if done naively.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"To train or serve such a model efficiently, three main hardware bottlenecks arise:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Compute\"}],\": speed of forward and backward passes\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Memory bandwidth\"}],\": rate of loading/storing model parameters\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Memory size\"}],\": capacity of the hardware accelerator\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Being first to launch an LLM can secure market share, so reducing training times is vital. Enter the modern AI accelerators, with specialized architectures to handle these tasks swiftly.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"introduction-to-gpus\",\"children\":[[[\"$\",\"a\",\"link-introduction-to-gpus\",{\"href\":\"#introduction-to-gpus\",\"className\":\"anchor\"}]],\"Introduction to GPUs\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"GPUs were originally for rendering graphics. Over the years, they’ve grown into the staple for massive parallelism needed in machine learning, scientific simulations, and cryptography. Key GPU design elements include:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"High Bandwidth Memory (HBM)\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Tensor Cores\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"high-bandwidth-memory-hbm\",\"children\":[[[\"$\",\"a\",\"link-high-bandwidth-memory-hbm\",{\"href\":\"#high-bandwidth-memory-hbm\",\"className\":\"anchor\"}]],\"High Bandwidth Memory (HBM)\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"HBM stacks memory vertically on the same package as the GPU, offering thousands of I/O pins. This yields far higher bandwidth—often terabytes per second—than standard GDDR. In deep learning, models constantly shuffle data to/from memory, so memory bandwidth can become the limiting factor. HBM helps ensure large models are not starved of data.\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"tensor-cores\",\"children\":[[[\"$\",\"a\",\"link-tensor-cores\",{\"href\":\"#tensor-cores\",\"className\":\"anchor\"}]],\"Tensor Cores\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Modern GPUs add \",[\"$\",\"strong\",null,{\"children\":\"Tensor Cores\"}],\", which accelerate matrix multiplications for deep learning. By using \",[\"$\",\"strong\",null,{\"children\":\"mixed-precision arithmetic\"}],\" (e.g., FP16 multiply with FP32 accumulate), Tensor Cores handle large matrix ops in a single operation. This massively boosts throughput on training and inference tasks—provided the memory system is fast enough to keep them busy.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"why-memory-bandwidth-matters\",\"children\":[[[\"$\",\"a\",\"link-why-memory-bandwidth-matters\",{\"href\":\"#why-memory-bandwidth-matters\",\"className\":\"anchor\"}]],\"Why Memory Bandwidth Matters\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Tensor Cores can do trillions of operations per second. If the memory subsystem can’t feed them data at a comparable rate, the GPU stalls. Hence, effective GPU design balances high compute with equally high memory bandwidth.\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"the-software-angle\",\"children\":[[[\"$\",\"a\",\"link-the-software-angle\",{\"href\":\"#the-software-angle\",\"className\":\"anchor\"}]],\"The Software Angle\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Alongside hardware improvements, software frameworks like PyTorch, TensorFlow, and JAX heavily influence training speed. These frameworks integrate with HPC libraries that manage GPU kernels, scheduling, and data transfer. The choice of distributed training strategies—model parallel, data parallel, or pipeline parallel—further impacts resource utilization. Even the best hardware can underperform if the software stack is not optimized for parallel operations and memory management.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"nvidia-h200-example\",\"children\":[[[\"$\",\"a\",\"link-nvidia-h200-example\",{\"href\":\"#nvidia-h200-example\",\"className\":\"anchor\"}]],\"NVIDIA H200 Example\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Let’s briefly consider the new NVIDIA H200 (\",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446\",\"children\":\"datasheet\"}],\").\"]}],\"\\n\",\"\\n\",[\"$\",\"img\",null,{\"src\":\"/images/efficiency/1_introduction/H200-datasheet.png\",\"alt\":\"H200 Datasheet\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"compute\",\"children\":[[[\"$\",\"a\",\"link-compute\",{\"href\":\"#compute\",\"className\":\"anchor\"}]],\"Compute\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"FP32 throughput\"}],\": ~989 TFLOPs. If you tried to train GPT-3 (~3.14e23 FLOPs total) on one H200 at full FP32 speed, you’d need on the order of \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"3.14\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"10\"}],[\"$\",\"mn\",null,{\"children\":\"23\"}]]}],[\"$\",\"mi\",null,{\"mathvariant\":\"normal\",\"children\":\"/\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mn\",null,{\"children\":\"989\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"10\"}],[\"$\",\"mn\",null,{\"children\":\"12\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mo\",null,{\"children\":\"≈\"}],[\"$\",\"mn\",null,{\"children\":\"3.17\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"msup\",null,{\"children\":[[\"$\",\"mn\",null,{\"children\":\"10\"}],[\"$\",\"mn\",null,{\"children\":\"8\"}]]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"3.14\\\\times10^{23} / (989\\\\times10^{12}) \\\\approx 3.17\\\\times10^8\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7278em\",\"verticalAlign\":\"-0.0833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"3.14\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.0641em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"0\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8141em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"23\"}]}]}]]}]}]}]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"/\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"989\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.0641em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"0\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8141em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"12\"}]}]}]]}]}]}]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"≈\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7278em\",\"verticalAlign\":\"-0.0833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"3.17\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.8141em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"0\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.8141em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.063em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mtight\",\"children\":\"8\"}]}]]}]}]}]}]}]]}]]}]]}]]}],\" seconds (~10 years).\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"FP8 Tensor Cores\"}],\": ~3958 TFLOPs. This quadruples raw throughput, reducing that naive 10-year figure to ~2.5 years—still impractical, but illustrative of what lower precision can do.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"memory-size\",\"children\":[[[\"$\",\"a\",\"link-memory-size\",{\"href\":\"#memory-size\",\"className\":\"anchor\"}]],\"Memory (Size)\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"The H200 has \",[\"$\",\"strong\",null,{\"children\":\"141GB\"}],\" of memory. GPT-3 in FP32 alone occupies ~700GB, so you’d need multiple GPUs just to hold the entire model in memory.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"memory-bandwidth\",\"children\":[[[\"$\",\"a\",\"link-memory-bandwidth\",{\"href\":\"#memory-bandwidth\",\"className\":\"anchor\"}]],\"Memory (Bandwidth)\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"The H200’s memory bandwidth is \",[\"$\",\"strong\",null,{\"children\":\"4.8TB/s\"}],\". Even if you have enough GPUs in parallel, training speed depends heavily on how fast you can transfer data.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"time-to-train-simplified\",\"children\":[[[\"$\",\"a\",\"link-time-to-train-simplified\",{\"href\":\"#time-to-train-simplified\",\"className\":\"anchor\"}]],\"Time to Train, Simplified\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"If one naive approach divides the model and data across (n) GPUs, each GPU’s training time might look like:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mtext\",null,{\"children\":\"Time\"}],[\"$\",\"mo\",null,{\"children\":\"∝\"}],[\"$\",\"mi\",null,{\"children\":\"n\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mtext\",null,{\"children\":\"compute time\"}],[\"$\",\"mo\",null,{\"children\":\"+\"}],[\"$\",\"mn\",null,{\"children\":\"2\"}],[\"$\",\"mo\",null,{\"children\":\"×\"}],[\"$\",\"mtext\",null,{\"children\":\"memory-load time\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\text{Time} \\\\propto n \\\\times (\\\\text{compute time} + 2 \\\\times \\\\text{memory-load time})\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"Time\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"∝\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6667em\",\"verticalAlign\":\"-0.0833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"n\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"compute time\"}]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"+\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.7278em\",\"verticalAlign\":\"-0.0833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"2\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"×\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"memory-load time\"}]}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]]}]]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"(The factor of 2 is for loading and unloading data chunks.) In practice, you use \",[\"$\",\"strong\",null,{\"children\":\"model parallelism\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"collective communication\"}],\" to share gradients, so it’s more complicated. But the takeaway is:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"More GPUs \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mtext\",null,{\"children\":\"  \"}],[\"$\",\"mo\",null,{\"children\":\"⟹\"}],[\"$\",\"mtext\",null,{\"children\":\"  \"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\implies\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.549em\",\"verticalAlign\":\"-0.024em\"}}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"⟹\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}]}]]}],\" more memory and more overall TFLOPs\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Lower precision \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mtext\",null,{\"children\":\"  \"}],[\"$\",\"mo\",null,{\"children\":\"⟹\"}],[\"$\",\"mtext\",null,{\"children\":\"  \"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\implies\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.549em\",\"verticalAlign\":\"-0.024em\"}}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"⟹\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}]}]]}],\"higher effective FLOPs\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Higher bandwidth \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mtext\",null,{\"children\":\"  \"}],[\"$\",\"mo\",null,{\"children\":\"⟹\"}],[\"$\",\"mtext\",null,{\"children\":\"  \"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\implies\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.549em\",\"verticalAlign\":\"-0.024em\"}}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"⟹\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}]}]]}],\" less stalling\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Thus, training can be shortened, at the cost of hardware expense and engineering complexity.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"distributed-hpc-clusters\",\"children\":[[[\"$\",\"a\",\"link-distributed-hpc-clusters\",{\"href\":\"#distributed-hpc-clusters\",\"className\":\"anchor\"}]],\"Distributed HPC Clusters\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Beyond a single GPU or even a single node, large-scale AI systems often run on multi-node high-performance computing (HPC) clusters. These clusters orchestrate many GPUs in parallel to reduce total training time. However, scaling to dozens or hundreds of GPUs introduces communication overheads—each GPU must exchange model gradients and parameters, typically using specialized libraries (e.g., NVIDIA NCCL).\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"As model sizes grow, the percentage of time GPUs spend simply synchronizing data can climb. Efficient collective operations, high-bandwidth interconnects like NVLink, InfiniBand, or 400Gb Ethernet, and custom topologies all help alleviate bottlenecks. Still, practitioners often find that real-world efficiency lags behind raw theoretical FLOPs, underscoring that “adding more GPUs” is no panacea without carefully managing data flow across the entire cluster.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"why-we-need-efficiency\",\"children\":[[[\"$\",\"a\",\"link-why-we-need-efficiency\",{\"href\":\"#why-we-need-efficiency\",\"className\":\"anchor\"}]],\"Why We Need Efficiency\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Running enormous models is neither cheap nor straightforward. Data centers pull megawatts of power, GPUs cost tens of thousands of dollars each, and the carbon footprints of large-scale training runs grow worrying. Researchers and enterprises want to optimize for minimal cost, faster turnaround, and smaller carbon impact without sacrificing model quality. Efficiency helps ensure that big ideas remain attainable—and not just for a handful of companies with colossal budgets.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We can’t simply turn the “scale knob” forever. Efficient methods—whether they be hardware-based (quantization, specialized memory) or algorithmic (pruning, better architectures, distributed training paradigms)—let us capture the benefits of scale while mitigating the downsides. It’s not just about racing to the largest parameter count; it’s about making the largest count feasible.\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"further-environmental-and-social-implications\",\"children\":[[[\"$\",\"a\",\"link-further-environmental-and-social-implications\",{\"href\":\"#further-environmental-and-social-implications\",\"className\":\"anchor\"}]],\"Further Environmental and Social Implications\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Massive AI training runs demand correspondingly massive energy inputs, straining power grids and increasing global carbon emissions. Some estimates suggest state-of-the-art models can produce emissions on par with entire vehicle lifecycles. These considerations motivate new research not just in hardware efficiency—like better memory systems and specialized circuits—but also in algorithmic techniques that can drastically cut training iterations. Efficiency, therefore, is about more than saving money or time. It aligns AI development with sustainability goals and can open the door for smaller organizations to contribute cutting-edge innovations without incurring prohibitive resource demands.\"}],\"\\n\",[\"$\",\"hr\",null,{}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"conclusion\",\"children\":[[[\"$\",\"a\",\"link-conclusion\",{\"href\":\"#conclusion\",\"className\":\"anchor\"}]],\"Conclusion\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This post spotlighted the critical pillars of AI efficiency: how bigger LLMs drive performance but impose severe hardware and cost constraints. We surveyed GPUs, memory bandwidth, and the interplay of precision, parameter count, and data size. While brute-force scaling can work, it collides with practical limits—training times stretch to years, hardware budgets skyrocket, and model footprints become colossal.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In subsequent posts, I will outline the broad strategies that aim to balance performance and pragmatism: pruning or sparsifying large models, automating architecture searches for optimal efficiency, quantizing parameters, distilling knowledge into smaller nets, refining transformers to handle longer contexts, and deploying LLMs at scale across distributed clusters. Each technique represents a piece of the puzzle: how do we keep pushing AI forward without exponentially inflating resource use?\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"By exploring these methods, we can uncover ways to sustain the breakneck pace of AI progress without abandoning reason—or our electric grid—in the process.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"tags\",\"children\":[[[\"$\",\"a\",\"link-tags\",{\"href\":\"#tags\",\"className\":\"anchor\"}]],\"Tags\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"#AI #efficiency #hardware #gpu\"}]]\n"])</script></body></html>